{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d2743d7-c381-4b41-9cb5-9a9e46e89150",
   "metadata": {},
   "source": [
    "**Q1. What is the Filter method in feature selection, and how does it work?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb32d4e5-547b-4964-ad3b-d922f7f711f3",
   "metadata": {},
   "source": [
    "The **Filter method** in feature selection is a technique used to select relevant features from a dataset based on statistical measures. This method evaluates the importance of each feature independently of any machine learning algorithm and ranks features based on their scores. The primary goal is to remove irrelevant or redundant features before model training.\n",
    "\n",
    "### How It Works:\n",
    "1. **Statistical Measures:** Filter methods use various statistical tests to evaluate the relationship between each feature and the target variable.\n",
    "   - **Correlation Coefficient:** Measures the linear relationship between continuous features and the target variable. For example, Pearson’s correlation coefficient.\n",
    "   - **Chi-Square Test:** Measures the association between categorical features and the target variable. Suitable for categorical data.\n",
    "   - **ANOVA (Analysis of Variance):** Compares the means of different groups to determine if at least one group mean is statistically different. Used for continuous features with categorical target variables.\n",
    "   - **Mutual Information:** Measures the amount of information obtained about one variable through another variable. Can handle both continuous and categorical data.\n",
    "\n",
    "2. **Ranking Features:** Features are ranked based on the chosen statistical measure. Higher scores indicate more important features.\n",
    "\n",
    "3. **Feature Selection:** A threshold or a fixed number of top-ranked features is selected to form the final feature set.\n",
    "\n",
    "### Example:\n",
    "Suppose you have a dataset with features: `age`, `income`, `education`, and `gender`, and the target variable is `purchased`.\n",
    "\n",
    "1. **Correlation Coefficient (for continuous features):**\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   from scipy.stats import pearsonr\n",
    "\n",
    "   df = pd.DataFrame({\n",
    "       'age': [25, 45, 35, 50, 23],\n",
    "       'income': [50000, 100000, 75000, 120000, 45000],\n",
    "       'purchased': [0, 1, 0, 1, 0]\n",
    "   })\n",
    "\n",
    "   corr_age, _ = pearsonr(df['age'], df['purchased'])\n",
    "   corr_income, _ = pearsonr(df['income'], df['purchased'])\n",
    "\n",
    "   print(f'Correlation with purchased - Age: {corr_age}, Income: {corr_income}')\n",
    "   ```\n",
    "\n",
    "2. **Chi-Square Test (for categorical features):**\n",
    "   ```python\n",
    "   from sklearn.feature_selection import chi2\n",
    "   import numpy as np\n",
    "\n",
    "   df['education'] = [1, 2, 1, 2, 1]  # Assuming 1: High School, 2: Bachelor\n",
    "   df['gender'] = [0, 1, 1, 0, 0]  # Assuming 0: Female, 1: Male\n",
    "\n",
    "   chi2_score, p_value = chi2(df[['education', 'gender']], df['purchased'])\n",
    "\n",
    "   print(f'Chi-Square Score: {chi2_score}, P-value: {p_value}')\n",
    "   ```\n",
    "\n",
    "3. **SelectKBest (Using Scikit-Learn):**\n",
    "   ```python\n",
    "   from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "   X = df[['age', 'income', 'education', 'gender']]\n",
    "   y = df['purchased']\n",
    "\n",
    "   selector = SelectKBest(score_func=f_classif, k=2)\n",
    "   X_new = selector.fit_transform(X, y)\n",
    "\n",
    "   print(X_new)\n",
    "   ```\n",
    "\n",
    "In this example, features are evaluated independently using statistical tests, and the most relevant ones are selected based on their scores. The Filter method is efficient and works well as a preprocessing step before applying more computationally intensive methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd30e53-ba9f-451b-99b8-7a813c6f477a",
   "metadata": {},
   "source": [
    "**Q2. How does the Wrapper method differ from the Filter method in feature selection?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80d91f5-4e20-4d6b-8a6f-f784856fee51",
   "metadata": {},
   "source": [
    "The **Wrapper method** and the **Filter method** are two different approaches to feature selection in machine learning. Here's how they differ:\n",
    "\n",
    "### Filter Method\n",
    "1. **Independence from Learning Algorithm:**\n",
    "   - The Filter method evaluates features based on statistical measures independently of any machine learning algorithm.\n",
    "   - Common techniques include correlation coefficients, Chi-Square tests, ANOVA, and mutual information.\n",
    "\n",
    "2. **Efficiency:**\n",
    "   - It is computationally efficient because it does not involve training models.\n",
    "   - It can quickly handle large datasets.\n",
    "\n",
    "3. **Feature Ranking:**\n",
    "   - Features are ranked based on their scores from the statistical tests, and a threshold or a fixed number of top-ranked features is selected.\n",
    "\n",
    "4. **Example:**\n",
    "   - Using Pearson's correlation to select features that have a high correlation with the target variable.\n",
    "   - Using Chi-Square tests for categorical data to select features with a strong association with the target variable.\n",
    "\n",
    "### Wrapper Method\n",
    "1. **Dependence on Learning Algorithm:**\n",
    "   - The Wrapper method evaluates subsets of features by actually training a model and measuring its performance.\n",
    "   - It uses a specific learning algorithm to assess the quality of the selected features.\n",
    "\n",
    "2. **Computational Complexity:**\n",
    "   - It is computationally intensive because it involves training and evaluating models multiple times.\n",
    "   - It is less suitable for very large datasets due to the high computational cost.\n",
    "\n",
    "3. **Feature Subset Selection:**\n",
    "   - The method searches for the best subset of features by evaluating different combinations.\n",
    "   - Common strategies include forward selection, backward elimination, and recursive feature elimination.\n",
    "\n",
    "4. **Example:**\n",
    "   - Forward selection starts with no features and adds features one by one, evaluating the model's performance at each step.\n",
    "   - Backward elimination starts with all features and removes them one by one, evaluating the model's performance at each step.\n",
    "   - Recursive feature elimination recursively removes the least important features and evaluates the model.\n",
    "\n",
    "### Comparison\n",
    "- **Efficiency vs. Accuracy:**\n",
    "  - **Filter Method:** More efficient but may not always select the best feature subset for a specific model.\n",
    "  - **Wrapper Method:** More accurate for a specific model but computationally expensive.\n",
    "\n",
    "- **Algorithm Independence vs. Dependence:**\n",
    "  - **Filter Method:** Independent of the learning algorithm.\n",
    "  - **Wrapper Method:** Dependent on the learning algorithm and its performance.\n",
    "\n",
    "- **Feature Selection Process:**\n",
    "  - **Filter Method:** Ranks features individually based on statistical scores.\n",
    "  - **Wrapper Method:** Considers feature subsets and evaluates their performance using a learning algorithm.\n",
    "\n",
    "### Example Code:\n",
    "#### Filter Method\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Load data\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Apply SelectKBest\n",
    "selector = SelectKBest(score_func=f_classif, k=2)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "print(X_new.shape)\n",
    "```\n",
    "\n",
    "#### Wrapper Method\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load data\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Apply RFE with logistic regression\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, n_features_to_select=2)\n",
    "X_rfe = rfe.fit_transform(X, y)\n",
    "\n",
    "print(X_rfe.shape)\n",
    "```\n",
    "\n",
    "In summary, the Filter method is efficient and model-agnostic, while the Wrapper method is more accurate for specific models but computationally intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae27562-6bc4-4c2d-be6b-c531690a8e44",
   "metadata": {},
   "source": [
    "**Q3. What are some common techniques used in Embedded feature selection methods?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0702fa-2c8f-4bb0-852c-a850805f2e54",
   "metadata": {},
   "source": [
    "Embedded feature selection methods incorporate the process of feature selection directly into the model training. These methods leverage the inherent properties of specific machine learning algorithms to select the most important features. Here are some common techniques used in embedded feature selection methods:\n",
    "\n",
    "### 1. **Regularization Methods**\n",
    "Regularization techniques add a penalty term to the objective function during model training, which helps in shrinking some feature coefficients to zero, effectively performing feature selection.\n",
    "\n",
    "- **Lasso (L1) Regularization:** Adds an L1 penalty to the loss function, which can shrink some coefficients to zero, thus selecting a subset of features.\n",
    "  ```python\n",
    "  from sklearn.linear_model import Lasso\n",
    "\n",
    "  model = Lasso(alpha=0.01)\n",
    "  model.fit(X, y)\n",
    "  selected_features = model.coef_ != 0\n",
    "  ```\n",
    "  \n",
    "- **Ridge (L2) Regularization:** Adds an L2 penalty to the loss function, which does not shrink coefficients to zero but helps in dealing with multicollinearity.\n",
    "  ```python\n",
    "  from sklearn.linear_model import Ridge\n",
    "\n",
    "  model = Ridge(alpha=0.01)\n",
    "  model.fit(X, y)\n",
    "  # Ridge does not directly select features but helps in regularization\n",
    "  ```\n",
    "\n",
    "- **Elastic Net:** Combines L1 and L2 penalties, allowing for both feature selection and regularization.\n",
    "  ```python\n",
    "  from sklearn.linear_model import ElasticNet\n",
    "\n",
    "  model = ElasticNet(alpha=0.01, l1_ratio=0.5)\n",
    "  model.fit(X, y)\n",
    "  selected_features = model.coef_ != 0\n",
    "  ```\n",
    "\n",
    "### 2. **Tree-Based Methods**\n",
    "Tree-based models inherently perform feature selection by splitting on the most important features.\n",
    "\n",
    "- **Decision Trees:** During the splitting process, features that provide the best split (based on criteria like Gini impurity or information gain) are selected.\n",
    "  ```python\n",
    "  from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "  model = DecisionTreeClassifier()\n",
    "  model.fit(X, y)\n",
    "  importances = model.feature_importances_\n",
    "  ```\n",
    "\n",
    "- **Random Forests:** Aggregate the feature importances from multiple decision trees to determine the most important features.\n",
    "  ```python\n",
    "  from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "  model = RandomForestClassifier()\n",
    "  model.fit(X, y)\n",
    "  importances = model.feature_importances_\n",
    "  ```\n",
    "\n",
    "- **Gradient Boosting:** Similar to Random Forests, it builds trees sequentially, focusing on the most important features to reduce the error.\n",
    "  ```python\n",
    "  from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "  model = GradientBoostingClassifier()\n",
    "  model.fit(X, y)\n",
    "  importances = model.feature_importances_\n",
    "  ```\n",
    "\n",
    "### 3. **Embedded Methods in Specific Algorithms**\n",
    "Some machine learning algorithms have built-in mechanisms for feature selection during training.\n",
    "\n",
    "- **Support Vector Machines (SVM) with L1 Regularization:** Uses L1 penalty to select a subset of features.\n",
    "  ```python\n",
    "  from sklearn.svm import LinearSVC\n",
    "\n",
    "  model = LinearSVC(C=0.01, penalty='l1', dual=False)\n",
    "  model.fit(X, y)\n",
    "  selected_features = model.coef_ != 0\n",
    "  ```\n",
    "\n",
    "- **Least Absolute Shrinkage and Selection Operator (Lasso):** A linear model with L1 regularization.\n",
    "  ```python\n",
    "  from sklearn.linear_model import Lasso\n",
    "\n",
    "  model = Lasso(alpha=0.01)\n",
    "  model.fit(X, y)\n",
    "  selected_features = model.coef_ != 0\n",
    "  ```\n",
    "\n",
    "### 4. **Gradient Boosted Trees**\n",
    "Gradient Boosted Trees like XGBoost and LightGBM provide feature importance scores that can be used to select features.\n",
    "  ```python\n",
    "  from xgboost import XGBClassifier\n",
    "\n",
    "  model = XGBClassifier()\n",
    "  model.fit(X, y)\n",
    "  importances = model.feature_importances_\n",
    "  ```\n",
    "\n",
    "### 5. **Regularized Regression Methods**\n",
    "- **Lasso Regression:** Regularized regression method that includes an L1 penalty to the loss function.\n",
    "  ```python\n",
    "  from sklearn.linear_model import Lasso\n",
    "\n",
    "  model = Lasso(alpha=0.01)\n",
    "  model.fit(X, y)\n",
    "  selected_features = model.coef_ != 0\n",
    "  ```\n",
    "\n",
    "In summary, embedded methods integrate feature selection with the model training process. Regularization techniques like Lasso and Elastic Net, tree-based methods like Random Forests and Gradient Boosting, and specific algorithms with built-in feature selection mechanisms are common techniques used in embedded feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b65b197-ee66-401e-b55c-378e0a6e1ec4",
   "metadata": {},
   "source": [
    "**Q4. What are some drawbacks of using the Filter method for feature selection?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3e60ac-62e6-49d8-b8d6-c40e59139af3",
   "metadata": {},
   "source": [
    "The Filter method for feature selection has several drawbacks despite its simplicity and efficiency. Here are some of the main drawbacks:\n",
    "\n",
    "### 1. **Ignores Feature Dependencies**\n",
    "- **Description:** The Filter method evaluates each feature individually, without considering interactions or dependencies between features.\n",
    "- **Impact:** This can result in selecting features that may be individually relevant but collectively redundant or irrelevant when used together.\n",
    "\n",
    "### 2. **Not Tailored to Specific Models**\n",
    "- **Description:** Filter methods do not take into account the specific model or algorithm that will ultimately be used for prediction.\n",
    "- **Impact:** The selected features might not be optimal for the chosen model, leading to suboptimal model performance.\n",
    "\n",
    "### 3. **May Miss Important Features**\n",
    "- **Description:** By relying on simple statistical metrics, Filter methods may overlook features that do not show a strong individual correlation with the target variable but are important in combination with other features.\n",
    "- **Impact:** Important features that contribute significantly to model performance might be excluded.\n",
    "\n",
    "### 4. **Threshold Selection Can Be Arbitrary**\n",
    "- **Description:** The criteria or thresholds used to select features (e.g., p-values, correlation coefficients) can be arbitrary and may not always lead to the best set of features.\n",
    "- **Impact:** Different threshold values can lead to different sets of selected features, adding an element of subjectivity.\n",
    "\n",
    "### 5. **Scalability Issues**\n",
    "- **Description:** For datasets with a very high number of features, computing the relevance scores for each feature can become computationally expensive.\n",
    "- **Impact:** The efficiency of the Filter method can degrade with extremely large feature sets.\n",
    "\n",
    "### 6. **Prone to Overfitting**\n",
    "- **Description:** Although less prone than Wrapper methods, Filter methods can still overfit if the selection criteria are too specific to the training data.\n",
    "- **Impact:** This can lead to poor generalization on unseen data.\n",
    "\n",
    "### 7. **Limited to Univariate Analysis**\n",
    "- **Description:** Most Filter methods perform univariate analysis, evaluating each feature independently.\n",
    "- **Impact:** Multivariate relationships and interactions between features are ignored, potentially missing out on complex patterns in the data.\n",
    "\n",
    "### 8. **Static and Non-Iterative**\n",
    "- **Description:** Once the features are selected using a Filter method, the selection process is not typically revisited.\n",
    "- **Impact:** This lack of iteration can lead to a less optimal feature set compared to methods that iteratively refine the feature selection based on model performance.\n",
    "\n",
    "### Example to Illustrate Some Drawbacks\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "# Apply Filter method (ANOVA F-test)\n",
    "selector = SelectKBest(score_func=f_classif, k=5)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "\n",
    "print(\"Selected Features:\", selected_features)\n",
    "```\n",
    "\n",
    "In this example:\n",
    "- **Ignore Dependencies:** The selected features are based solely on their individual scores, ignoring any potential interactions between features.\n",
    "- **Not Model-Specific:** The selection process does not consider which model will be used, so it might not be optimal for any specific algorithm.\n",
    "\n",
    "In conclusion, while Filter methods are simple and computationally efficient, they have significant drawbacks, including ignoring feature dependencies, not being tailored to specific models, and potentially missing important features. These limitations must be considered when choosing a feature selection method for a particular machine learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f77e3f-8cdc-431a-8d38-01950acd0660",
   "metadata": {},
   "source": [
    "**Q5. In which situations would you prefer using the Filter method over the Wrapper method for feature\n",
    "selection?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af426d5-4477-44fe-b6a7-0d999e28ee68",
   "metadata": {},
   "source": [
    "The Filter method for feature selection is preferred over the Wrapper method in several situations, primarily due to its simplicity, efficiency, and independence from the learning algorithm. Here are some scenarios where the Filter method is advantageous:\n",
    "\n",
    "### 1. **Large Datasets with High Dimensionality**\n",
    "- **Description:** When dealing with datasets that have a very large number of features, the computational efficiency of the Filter method is a significant advantage.\n",
    "- **Reason:** The Filter method evaluates features independently and can quickly reduce the feature space, making subsequent analysis and model training more manageable.\n",
    "\n",
    "### 2. **Preprocessing Step**\n",
    "- **Description:** The Filter method is often used as a preprocessing step to eliminate irrelevant or redundant features before applying more sophisticated feature selection techniques or model training.\n",
    "- **Reason:** This initial filtering helps to reduce the computational load and complexity, making it easier to apply other methods or algorithms.\n",
    "\n",
    "### 3. **Simplicity and Speed**\n",
    "- **Description:** When quick, straightforward feature selection is needed, especially in exploratory data analysis (EDA) or initial model prototyping.\n",
    "- **Reason:** Filter methods are fast and easy to implement, providing a quick way to understand which features might be relevant.\n",
    "\n",
    "### 4. **Baseline Feature Selection**\n",
    "- **Description:** For creating a baseline model where the primary goal is to quickly get a working model and then iteratively improve it.\n",
    "- **Reason:** Filter methods can provide a good starting point by quickly reducing the feature set to a manageable size.\n",
    "\n",
    "### 5. **Model Agnostic Approach**\n",
    "- **Description:** When the feature selection process needs to be independent of the specific machine learning algorithm being used.\n",
    "- **Reason:** Filter methods do not rely on a particular model and can be applied regardless of the learning algorithm, making them versatile.\n",
    "\n",
    "### 6. **Avoiding Overfitting**\n",
    "- **Description:** In cases where overfitting is a concern, and there is a need to avoid the model-specific bias introduced by Wrapper methods.\n",
    "- **Reason:** Filter methods are less prone to overfitting since they do not involve the iterative training of a model.\n",
    "\n",
    "### 7. **Resource Constraints**\n",
    "- **Description:** When there are limitations on computational resources, such as processing power or memory.\n",
    "- **Reason:** Filter methods are computationally less intensive compared to Wrapper methods, which require multiple rounds of model training.\n",
    "\n",
    "### Example to Illustrate Preferred Use Cases\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "# Apply Filter method (Chi-square test)\n",
    "selector = SelectKBest(score_func=chi2, k=2)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "\n",
    "print(\"Selected Features:\", selected_features)\n",
    "```\n",
    "\n",
    "In this example:\n",
    "- **Large Datasets with High Dimensionality:** If the dataset had many more features, the Filter method would efficiently reduce the feature set.\n",
    "- **Preprocessing Step:** This method can be used as a preliminary step before more complex feature selection.\n",
    "- **Simplicity and Speed:** It quickly identifies the top features without model-specific iterations.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The Filter method is particularly useful in scenarios where computational efficiency, simplicity, and independence from the learning algorithm are essential. It serves well in preprocessing, handling high-dimensional data, creating baseline models, and situations where resource constraints are a concern. However, it may not be suitable for capturing feature interactions or dependencies that more sophisticated methods like Wrappers can handle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919cdb15-2ac8-4648-9eb8-9a5cefad7fdf",
   "metadata": {},
   "source": [
    "**Q6. In a telecom company, you are working on a project to develop a predictive model for customer churn.\n",
    "You are unsure of which features to include in the model because the dataset contains several different\n",
    "ones. Describe how you would choose the most pertinent attributes for the model using the Filter Method.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b98d7eb-6d8e-4586-807f-9d0235693993",
   "metadata": {},
   "source": [
    "To choose the most pertinent attributes for a predictive model of customer churn in a telecom company using the Filter Method, you would follow a systematic process that involves statistical techniques to evaluate the relevance of each feature independently of the machine learning algorithm. Here is a step-by-step approach:\n",
    "\n",
    "### Step 1: Understand the Dataset\n",
    "- **Objective:** Get a clear understanding of the dataset, including the target variable (customer churn) and the features available.\n",
    "- **Action:** Load the dataset and perform an initial exploratory data analysis (EDA) to understand the nature of each feature (e.g., numerical, categorical) and the distribution of the target variable.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('telecom_churn_data.csv')\n",
    "\n",
    "# Initial exploration\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "```\n",
    "\n",
    "### Step 2: Preprocess the Data\n",
    "- **Objective:** Prepare the data by handling missing values, encoding categorical variables, and scaling numerical features if necessary.\n",
    "- **Action:** Apply appropriate preprocessing techniques to ensure the dataset is clean and ready for feature selection.\n",
    "\n",
    "```python\n",
    "# Handle missing values\n",
    "df.fillna(df.median(), inplace=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# Feature and target separation\n",
    "X = df.drop('churn', axis=1)\n",
    "y = df['churn']\n",
    "```\n",
    "\n",
    "### Step 3: Apply the Filter Method\n",
    "- **Objective:** Use statistical techniques to evaluate the relevance of each feature with respect to the target variable (customer churn).\n",
    "- **Action:** Select a suitable statistical test based on the type of features (e.g., chi-square for categorical features, ANOVA F-test for numerical features).\n",
    "\n",
    "#### Example: Using Chi-Square Test for Categorical Features\n",
    "```python\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Apply Chi-Square test\n",
    "selector = SelectKBest(score_func=chi2, k='all')\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "# Get the scores for each feature\n",
    "scores = selector.scores_\n",
    "\n",
    "# Create a DataFrame to display feature scores\n",
    "feature_scores = pd.DataFrame({'Feature': X.columns, 'Score': scores})\n",
    "feature_scores = feature_scores.sort_values(by='Score', ascending=False)\n",
    "\n",
    "print(feature_scores)\n",
    "```\n",
    "\n",
    "#### Example: Using ANOVA F-test for Numerical Features\n",
    "```python\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Apply ANOVA F-test\n",
    "selector = SelectKBest(score_func=f_classif, k='all')\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "# Get the scores for each feature\n",
    "scores = selector.scores_\n",
    "\n",
    "# Create a DataFrame to display feature scores\n",
    "feature_scores = pd.DataFrame({'Feature': X.columns, 'Score': scores})\n",
    "feature_scores = feature_scores.sort_values(by='Score', ascending=False)\n",
    "\n",
    "print(feature_scores)\n",
    "```\n",
    "\n",
    "### Step 4: Select Top Features\n",
    "- **Objective:** Identify and select the most relevant features based on the scores obtained from the statistical tests.\n",
    "- **Action:** Choose a threshold or a fixed number of top features to retain for model training.\n",
    "\n",
    "```python\n",
    "# Select top 10 features (example threshold)\n",
    "top_features = feature_scores.head(10)['Feature'].values\n",
    "\n",
    "# Create a new DataFrame with the top features\n",
    "X_top = X[top_features]\n",
    "\n",
    "print(X_top.head())\n",
    "```\n",
    "\n",
    "### Step 5: Validate the Selected Features\n",
    "- **Objective:** Ensure the selected features improve model performance and are relevant for predicting customer churn.\n",
    "- **Action:** Train a preliminary model using the selected features and evaluate its performance using appropriate metrics (e.g., accuracy, precision, recall, F1 score).\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_top, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "### Conclusion\n",
    "By following this process, you can efficiently use the Filter Method to select the most pertinent features for your predictive model of customer churn. This method ensures that you retain only the most relevant features, which can lead to better model performance and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09d5256-a91e-4c30-901b-86e3581dee5e",
   "metadata": {},
   "source": [
    "**Q7. You are working on a project to predict the outcome of a soccer match. You have a large dataset with\n",
    "many features, including player statistics and team rankings. Explain how you would use the Embedded\n",
    "method to select the most relevant features for the model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d38ea2-9777-4005-95ad-669631fdccde",
   "metadata": {},
   "source": [
    "In the context of predicting the outcome of a soccer match using a large dataset with many features, including player statistics and team rankings, the Embedded method refers to techniques that perform feature selection as part of the model training process itself. This is typically done by algorithms that have built-in mechanisms to assess and rank the importance of features while fitting the model.\n",
    "\n",
    "Here’s how you could use the Embedded method to select the most relevant features for your soccer match outcome prediction model:\n",
    "\n",
    "1. **Choose a Model with Embedded Feature Selection**: Select a machine learning algorithm that inherently performs feature selection as it trains. Examples include:\n",
    "   - **Lasso Regression**: Uses L1 regularization, which penalizes the absolute size of coefficients, leading to sparse solutions where less important features have coefficients that are reduced to zero.\n",
    "   - **Elastic Net**: Combines L1 and L2 regularization, offering a compromise between Lasso and Ridge regression, suitable when there are correlations among features.\n",
    "   - **Decision Trees**: Models like Random Forests or Gradient Boosting Machines (GBM) inherently perform feature selection by selecting features that best split the data at each node.\n",
    "\n",
    "2. **Train the Model**: Fit the chosen model on your dataset that includes all potential features, such as player statistics and team rankings.\n",
    "\n",
    "3. **Feature Importance Evaluation**: During the training process, the model assesses the importance of each feature based on how much each feature contributes to improving the model's performance. This contribution is often measured by metrics such as:\n",
    "   - **Coefficients**: For linear models like Lasso Regression, features with non-zero coefficients after regularization are considered important.\n",
    "   - **Feature Importances**: For decision tree-based models (e.g., Random Forests, GBM), features that lead to the greatest reduction in impurity (like Gini impurity or entropy) across all decision trees are deemed more important.\n",
    "\n",
    "4. **Select Relevant Features**: After training, you can extract or select the features that are deemed most important according to the chosen metric of feature importance.\n",
    "\n",
    "5. **Refine and Validate**: Refine your model by iterating on feature selection if needed, and validate its performance using appropriate evaluation metrics (e.g., accuracy, precision, recall, F1-score) on a validation set or through cross-validation.\n",
    "\n",
    "By using an Embedded method like Lasso Regression, Elastic Net, or decision tree-based models, you can effectively select the most relevant features from your dataset for predicting soccer match outcomes. This approach helps in reducing overfitting, improving model interpretability, and potentially enhancing prediction accuracy by focusing on the most informative features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be24201-8e5b-4914-bd98-0f65b54f6f26",
   "metadata": {},
   "source": [
    "**Q8. You are working on a project to predict the price of a house based on its features, such as size, location,\n",
    "and age. You have a limited number of features, and you want to ensure that you select the most important\n",
    "ones for the model. Explain how you would use the Wrapper method to select the best set of features for the\n",
    "predictor.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c705dbed-807b-41a7-ac51-9762711e3529",
   "metadata": {},
   "source": [
    "In the context of predicting house prices with a limited number of features like size, location, and age, the Wrapper method for feature selection involves evaluating different subsets of features to determine which combination yields the best predictive performance for the model. Here’s how you can use the Wrapper method to select the best set of features:\n",
    "\n",
    "1. **Choose a Subset of Features**: Define the initial set of features you want to evaluate. In your case, this might include size (square footage), location (neighborhood or city), and age of the house.\n",
    "\n",
    "2. **Select a Performance Metric**: Decide on a metric to evaluate the performance of your predictive model. Common metrics for regression tasks like predicting house prices include Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), or \\( R^2 \\) score.\n",
    "\n",
    "3. **Subset Evaluation**: Use a search strategy to evaluate different subsets of your initial features. Two commonly used methods for Wrapper feature selection are:\n",
    "   - **Forward Selection**: Start with an empty set of features and iteratively add one feature at a time, choosing the feature that improves the model performance the most until a stopping criterion is met.\n",
    "   - **Backward Elimination**: Begin with all features and iteratively remove one feature at a time, eliminating the feature that has the least impact on model performance until a stopping criterion is met.\n",
    "\n",
    "Certainly! Continuing from point number 4:\n",
    "\n",
    "4. **Train and Validate Models**: For each subset of features evaluated during the search process, train a predictive model (such as linear regression, decision trees, or other suitable algorithms) using the selected features. \n",
    "\n",
    "   - **Training**: Use the training dataset to fit the model with the chosen subset of features.\n",
    "   - **Validation**: Evaluate the model's performance using the selected performance metric (e.g., MSE, RMSE, MAE, $ R^2 $ score) on a separate validation dataset or through cross-validation. This step ensures that the model's performance is assessed independently of the training data, helping to gauge its generalization ability.\n",
    "\n",
    "5. **Select the Best Subset**: Compare the performance of each subset of features based on the chosen metric. The subset that yields the best performance metric (e.g., lowest MSE, highest \\( R^2 \\) score) is selected as the optimal set of features for predicting house prices.\n",
    "\n",
    "6. **Refinement and Validation**: Optionally, refine the selected subset further based on domain knowledge or additional criteria, and validate the final model on a test dataset to confirm its robustness and reliability for predicting house prices in real-world scenarios.\n",
    "\n",
    "By systematically evaluating different subsets of features through methods like Forward Selection or Backward Elimination within the Wrapper method framework, you can identify and leverage the most informative features for accurately predicting house prices while ensuring the model's performance is optimized and validated effectively.     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
