{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78531e23-01b4-4583-ba3e-fa0eb92927d4",
   "metadata": {},
   "source": [
    "**Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95530024-cbc3-48dc-8695-41c302cf03ca",
   "metadata": {},
   "source": [
    "Min-Max scaling is a technique used in data preprocessing to transform numerical features to a common scale. It rescales the values of numeric features to a range, usually between 0 and 1. This transformation is useful when working with algorithms that require all features to have the same scale, such as gradient descent-based algorithms, support vector machines, and k-nearest neighbors.\n",
    "\n",
    "Here's how Min-Max scaling is applied:\n",
    "\n",
    "1. **Formula**: The formula for Min-Max scaling of a feature $ X $ is:\n",
    "   $\n",
    "   X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\n",
    "   $\n",
    "   where $ X_{\\text{min}} $ and $ X_{\\text{max}} $ are the minimum and maximum values of $ X $ in the dataset, respectively.\n",
    "\n",
    "2. **Example**: Suppose we have a dataset containing a feature $ X $ with values ranging from 10 to 50. We want to scale this feature using Min-Max scaling.\n",
    "\n",
    "   - $ X = [10, 20, 30, 40, 50] $\n",
    "   - $ X_{\\text{min}} = 10 $\n",
    "   - $ X_{\\text{max}} = 50 $\n",
    "\n",
    "   Applying the Min-Max scaling formula:\n",
    "   - For $ X = 10 $:\n",
    "     $\n",
    "     X_{\\text{scaled}} = \\frac{10 - 10}{50 - 10} = \\frac{0}{40} = 0\n",
    "     $\n",
    "   - For $ X = 20 $):\n",
    "     $\n",
    "     X_{\\text{scaled}} = \\frac{20 - 10}{50 - 10} = \\frac{10}{40} = 0.25\n",
    "     $\n",
    "   - For $ X = 30 $:\n",
    "     $\n",
    "     X_{\\text{scaled}} = \\frac{30 - 10}{50 - 10} = \\frac{20}{40} = 0.5\n",
    "     $\n",
    "   - For $ X = 40 $:\n",
    "     $\n",
    "     X_{\\text{scaled}} = \\frac{40 - 10}{50 - 10} = \\frac{30}{40} = 0.75\n",
    "     $\n",
    "   - For $ X = 50 $:\n",
    "     $\n",
    "     X_{\\text{scaled}} = \\frac{50 - 10}{50 - 10} = \\frac{40}{40} = 1\n",
    "     $\n",
    "\n",
    "   Thus, after Min-Max scaling, the scaled values of $ X $ would be $ [0, 0.25, 0.5, 0.75, 1] $.\n",
    "\n",
    "This scaling method ensures that all features are on the same scale, preventing certain features from dominating due to their larger numerical ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71100e38-c817-4a9d-a7ee-f6994e97fcaa",
   "metadata": {},
   "source": [
    "**Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af402aa0-c789-4965-b0c8-55ba04004a09",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as normalization or L2 normalization, is another method used for feature scaling in data preprocessing. Unlike Min-Max scaling, which scales features to a fixed range (typically between 0 and 1), Unit Vector scaling adjusts the values of a feature vector so that the vector norm (magnitude) equals 1. This technique is particularly useful when the direction of the data matters more than its magnitude.\n",
    "\n",
    "Here's how Unit Vector scaling works:\n",
    "\n",
    "1. **Formula**: The formula for Unit Vector scaling of a feature vector $ \\mathbf{x} = [x_1, x_2, \\ldots, x_n] $ is:\n",
    "   $\n",
    "   \\mathbf{x}_{\\text{scaled}} = \\frac{\\mathbf{x}}{||\\mathbf{x}||}\n",
    "   $\n",
    "   where $ ||\\mathbf{x}|| $ denotes the Euclidean norm (magnitude) of the vector $ \\mathbf{x} $, given by $ ||\\mathbf{x}|| = \\sqrt{x_1^2 + x_2^2 + \\ldots + x_n^2} $.\n",
    "\n",
    "2. **Example**: Suppose we have a feature vector $ \\mathbf{x} = [3, 4] $. We want to scale this vector using Unit Vector scaling.\n",
    "\n",
    "   - Compute the Euclidean norm $ ||\\mathbf{x}|| $:\n",
    "     $\n",
    "     ||\\mathbf{x}|| = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5\n",
    "     $\n",
    "\n",
    "   - Apply Unit Vector scaling:\n",
    "     $\n",
    "     \\mathbf{x}_{\\text{scaled}} = \\frac{\\mathbf{x}}{||\\mathbf{x}||} = \\frac{[3, 4]}{5} = [0.6, 0.8]\n",
    "     $\n",
    "\n",
    "   After Unit Vector scaling, the scaled vector $( \\mathbf{x}_{\\text{scaled}} $ has a magnitude (norm) of 1, and its direction remains the same relative to the original vector $ \\mathbf{x} $.\n",
    "\n",
    "### Differences from Min-Max Scaling:\n",
    "- **Range of Values**: Min-Max scaling scales values to a fixed range (e.g., between 0 and 1), while Unit Vector scaling adjusts the vector to have a norm of 1.\n",
    "- **Effect on Magnitude**: Min-Max scaling preserves the magnitude and distribution of values within the original range, whereas Unit Vector scaling preserves the direction of the vector but changes its magnitude to 1.\n",
    "- **Application**: Min-Max scaling is often used when the absolute values of features are important and need to be on the same scale, while Unit Vector scaling is more suitable when the direction of the data relative to the origin is significant, such as in cosine similarity calculations or in algorithms where distance or angle matters more than magnitude.\n",
    "\n",
    "In summary, Unit Vector scaling is a normalization technique that adjusts feature vectors to have a constant norm (magnitude), making it useful in scenarios where the relative direction of vectors is more important than their actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a059cfc9-29bd-40c2-92f7-65ff4b8e60e8",
   "metadata": {},
   "source": [
    "**Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdbd710-9430-4f91-8adc-496e8a642898",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a statistical technique used for reducing the dimensionality of data while preserving its key features. It achieves this by transforming the original features into a new set of orthogonal components called principal components. These components are ordered by the amount of variance they explain in the data, with the first principal component capturing the maximum variance.\n",
    "\n",
    "### How PCA Works:\n",
    "\n",
    "1. **Covariance Matrix**: PCA starts by computing the covariance matrix of the data, which represents the relationships between the different features.\n",
    "\n",
    "2. **Eigen decomposition**: It then performs eigen decomposition or singular value decomposition (SVD) on the covariance matrix to extract the principal components. These components are eigenvectors that correspond to the eigenvalues (which represent the variance explained) of the covariance matrix.\n",
    "\n",
    "3. **Dimensionality Reduction**: PCA selects a subset of these principal components that explain most of the variance in the data. By projecting the original data onto these principal components, PCA effectively reduces the dimensionality of the dataset while retaining as much variance as possible.\n",
    "\n",
    "### Example Illustration:\n",
    "\n",
    "Suppose we have a dataset with two features, $ X_1 $ and $ X_2 $, and we want to reduce it to a single dimension using PCA.\n",
    "\n",
    "1. **Dataset**:\n",
    "   $\n",
    "   \\begin{bmatrix}\n",
    "   1 & 1 \\\\\n",
    "   2 & 3 \\\\\n",
    "   3 & 4 \\\\\n",
    "   4 & 6 \\\\\n",
    "   5 & 5 \\\\\n",
    "   \\end{bmatrix}\n",
    "   $\n",
    "\n",
    "2. **Step-by-Step PCA**:\n",
    "\n",
    "   - **Centering the Data**: Subtract the mean from each feature to center the data.\n",
    "     $\n",
    "     \\text{Mean of } X_1 = \\frac{1 + 2 + 3 + 4 + 5}{5} = 3, \\quad \\text{Mean of } X_2 = \\frac{1 + 3 + 4 + 6 + 5}{5} = 3.8\n",
    "     $\n",
    "     Centered data:\n",
    "     $\n",
    "     \\begin{bmatrix}\n",
    "     -2 & -2.8 \\\\\n",
    "     -1 & -0.8 \\\\\n",
    "     0 & 0.2 \\\\\n",
    "     1 & 2.2 \\\\\n",
    "     2 & 1.2 \\\\\n",
    "     \\end{bmatrix}\n",
    "     $\n",
    "\n",
    "   - **Compute Covariance Matrix**: \n",
    "     $\n",
    "     \\text{Covariance Matrix} = \\frac{1}{n-1} \\cdot \\text{Centered Data}^T \\cdot \\text{Centered Data}\n",
    "     $\n",
    "     Compute:\n",
    "     $\n",
    "     \\begin{bmatrix}\n",
    "     2.5 & 3.0 \\\\\n",
    "     3.0 & 4.3 \\\\\n",
    "     \\end{bmatrix}\n",
    "     $\n",
    "\n",
    "   - **Eigen decomposition**: Compute eigenvalues and eigenvectors of the covariance matrix.\n",
    "     $\n",
    "     \\text{Eigenvalues: } \\lambda_1 = 0.05, \\quad \\lambda_2 = 6.75\n",
    "     $\n",
    "     $\n",
    "     \\text{Eigenvectors: } \\mathbf{v}_1 = \\begin{bmatrix} -0.83 \\\\ 0.55 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} 0.55 \\\\ 0.83 \\end{bmatrix}\n",
    "     $\n",
    "\n",
    "   - **Select Principal Components**: Choose the principal component(s) that explain most of the variance. In this case, $ \\mathbf{v}_2 $ (corresponding to $ \\lambda_2 $) explains more variance.\n",
    "\n",
    "   - **Projection**: Project the original data onto the selected principal component(s).\n",
    "     $\n",
    "     \\text{Projected Data} = \\text{Centered Data} \\cdot \\mathbf{v}_2 = \\begin{bmatrix} -2 & -2.8 \\\\ -1 & -0.8 \\\\ 0 & 0.2 \\\\ 1 & 2.2 \\\\ 2 & 1.2 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.55 \\\\ 0.83 \\end{bmatrix}\n",
    "     $\n",
    "     Resulting in:\n",
    "     $\n",
    "     \\begin{bmatrix}\n",
    "     -3.03 \\\\\n",
    "     -1.49 \\\\\n",
    "     0.11 \\\\\n",
    "     2.17 \\\\\n",
    "     1.68 \\\\\n",
    "     \\end{bmatrix}\n",
    "     $\n",
    "\n",
    "   - **Dimensionality Reduction**: The projected data has been reduced from two dimensions to one dimension while retaining the variance in the data that is captured by the principal component $ \\mathbf{v}_2 $.\n",
    "\n",
    "PCA is widely used in various fields, including image processing, genetics, and finance, where reducing the dimensionality of data can lead to simpler models, easier visualization, and improved computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1a47d1-dcc2-4341-bec1-2643a74411da",
   "metadata": {},
   "source": [
    "**Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14727da-abad-40ab-8f48-339d10c81336",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is closely related to feature extraction, as it can be used to derive a smaller set of features (principal components) from a larger set of possibly correlated features. Feature extraction, in general, refers to the process of transforming raw data into a reduced feature set that still captures the essential information present in the original data. PCA achieves this by identifying the directions (principal components) along which the data varies the most.\n",
    "\n",
    "### Relationship between PCA and Feature Extraction:\n",
    "\n",
    "1. **Dimensionality Reduction**: PCA is primarily used for dimensionality reduction by transforming a dataset of possibly correlated features into a set of linearly uncorrelated components (principal components). These components are ordered by the amount of variance they explain in the data, with the first few components capturing the most variance.\n",
    "\n",
    "2. **Feature Extraction**: In the context of PCA, feature extraction involves selecting a subset of the principal components that explain most of the variance in the data. These principal components can then be used as the new features to represent the data in a lower-dimensional space.\n",
    "\n",
    "### How PCA can be used for Feature Extraction:\n",
    "\n",
    "Let's illustrate this with an example:\n",
    "\n",
    "#### Example Illustration:\n",
    "\n",
    "Suppose we have a dataset with three features $ X_1, X_2, $ and $ X_3 $. We want to use PCA for feature extraction to reduce the dimensionality of the dataset.\n",
    "\n",
    "1. **Dataset**:\n",
    "   $\n",
    "   \\begin{bmatrix}\n",
    "   1 & 2 & 3 \\\\\n",
    "   2 & 3 & 4 \\\\\n",
    "   3 & 4 & 5 \\\\\n",
    "   4 & 5 & 6 \\\\\n",
    "   5 & 6 & 7 \\\\\n",
    "   \\end{bmatrix}\n",
    "   $\n",
    "\n",
    "2. **Apply PCA**:\n",
    "\n",
    "   - **Centering the Data**: Subtract the mean from each feature to center the data.\n",
    "\n",
    "   - **Compute Covariance Matrix**: Compute the covariance matrix of the centered data.\n",
    "\n",
    "   - **Eigen decomposition**: Compute the eigenvalues and eigenvectors of the covariance matrix.\n",
    "\n",
    "   - **Select Principal Components**: Choose the principal components that explain most of the variance. For instance, if the first two principal components explain a significant amount of variance, we might select them.\n",
    "\n",
    "   - **Projection**: Project the original data onto the selected principal components to obtain the transformed (extracted) features.\n",
    "\n",
    "#### Example Calculation (Simplified):\n",
    "\n",
    "For simplicity, let's assume after PCA, we find that the first principal component $ \\mathbf{v}_1 $ explains 80% of the variance, and the second principal component $ \\mathbf{v}_2 $ explains 15% of the variance.\n",
    "\n",
    "- **Principal Components**:\n",
    "  $\n",
    "  \\mathbf{v}_1 = \\begin{bmatrix} 0.5 \\\\ 0.6 \\\\ 0.7 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} -0.6 \\\\ 0.7 \\\\ -0.3 \\end{bmatrix}\n",
    "  $\n",
    "\n",
    "- **Projected Data**: Project the original data onto the selected principal components.\n",
    "\n",
    "  If we choose to keep the first two principal components $ \\mathbf{v}_1 $ and $ \\mathbf{v}_2 $:\n",
    "\n",
    "  $\n",
    "  \\text{Projected Data} = \\text{Original Data} \\cdot \\begin{bmatrix} \\mathbf{v}_1 & \\mathbf{v}_2 \\end{bmatrix}\n",
    "  $\n",
    "\n",
    "  The resulting projected data will have reduced dimensionality (in this case, from 3 features to 2 principal components).\n",
    "\n",
    "### Summary:\n",
    "\n",
    "PCA is used for feature extraction by transforming the original features into a smaller set of principal components that retain the essential variance of the data. These principal components can serve as new features in subsequent modeling tasks, effectively reducing the dimensionality of the dataset while preserving most of the information. This process helps in simplifying models, improving computational efficiency, and potentially enhancing interpretability and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d977583-e31a-4754-827e-fad79b8f0fb7",
   "metadata": {},
   "source": [
    "**Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5db3d1-e4d5-4300-9fdb-45b563d391ae",
   "metadata": {},
   "source": [
    "To preprocess the data for building a recommendation system for a food delivery service using Min-Max scaling, follow these steps:\n",
    "\n",
    "1. **Understand the Dataset**: First, understand the range and distribution of the features you have. In your case, the features include price, rating, and delivery time. These features likely have different scales and units (e.g., price in dollars, rating on a scale, delivery time in minutes).\n",
    "\n",
    "2. **Apply Min-Max Scaling**:\n",
    "   - **Identify Min and Max Values**: Compute the minimum $( X_{\\text{min}} )$ and maximum $( X_{\\text{max}} )$ values for each feature across the dataset.\n",
    "   \n",
    "   - **Scale Each Feature**: For each feature $ X $, apply the Min-Max scaling transformation:\n",
    "     $\n",
    "     X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\n",
    "     $\n",
    "     This scales the values of $ X $ to a range between 0 and 1, where $ X_{\\text{min}} $ and $ X_{\\text{max}} $ are the minimum and maximum values of $ X $ in the dataset, respectively.\n",
    "\n",
    "3. **Implementation Example**:\n",
    "   - **Price**: Suppose the price ranges from $5 to $50 in your dataset. After Min-Max scaling:\n",
    "     $\n",
    "     X_{\\text{scaled}} = \\frac{\\text{price} - 5}{50 - 5}\n",
    "     $\n",
    "     This transformation ensures that prices are scaled proportionally between 0 and 1.\n",
    "   \n",
    "   - **Rating**: If ratings range from 1 to 5, after Min-Max scaling:\n",
    "     $\n",
    "     X_{\\text{scaled}} = \\frac{\\text{rating} - 1}{5 - 1}\n",
    "     $\n",
    "     This normalizes ratings to a scale between 0 and 1.\n",
    "   \n",
    "   - **Delivery Time**: Suppose delivery times range from 20 to 60 minutes. After Min-Max scaling:\n",
    "     $\n",
    "     X_{\\text{scaled}} = \\frac{\\text{delivery time} - 20}{60 - 20}\n",
    "     $\n",
    "     This scales delivery times to a range between 0 and 1.\n",
    "\n",
    "4. **Normalization Benefits**: Min-Max scaling ensures that all features are on the same scale, preventing any single feature from dominating due to its larger numerical range. This normalization is particularly useful for recommendation systems where features might have different units and scales but need to be comparably weighted during recommendation calculations.\n",
    "\n",
    "5. **Implementation in Code**: Use libraries such as scikit-learn in Python to implement Min-Max scaling efficiently. Here’s a basic example using scikit-learn's `MinMaxScaler`:\n",
    "\n",
    "   ```python\n",
    "   from sklearn.preprocessing import MinMaxScaler\n",
    "   \n",
    "   # Assuming 'data' is your dataset containing price, rating, delivery time, etc.\n",
    "   scaler = MinMaxScaler()\n",
    "   scaled_data = scaler.fit_transform(data)\n",
    "   ```\n",
    "\n",
    "   This `scaled_data` will contain your features scaled between 0 and 1, ready to be used in your recommendation system algorithm.\n",
    "\n",
    "By applying Min-Max scaling to your dataset, you ensure that the features are appropriately scaled and prepared for modeling, facilitating the development of an effective recommendation system for the food delivery service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533be4f5-c49e-48cf-bae8-40fae9997fec",
   "metadata": {},
   "source": [
    "**Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af789330-225b-40b1-a263-9e03c662839c",
   "metadata": {},
   "source": [
    "When working with a large dataset containing many features for predicting stock prices, PCA (Principal Component Analysis) can be a valuable technique for reducing the dimensionality while retaining the most important information. Here’s how you can use PCA in the context of predicting stock prices:\n",
    "\n",
    "### Steps to Use PCA for Dimensionality Reduction:\n",
    "\n",
    "1. **Data Preprocessing**:\n",
    "   - **Normalization**: Ensure all features are on a similar scale. This is crucial for PCA, as it operates based on variance, and features with larger scales can dominate the principal components.\n",
    "\n",
    "2. **Apply PCA**:\n",
    "   - **Compute Covariance Matrix**: Calculate the covariance matrix of your normalized dataset. This matrix helps identify how different features vary together.\n",
    "   \n",
    "   - **Eigen Decomposition**: Perform eigen decomposition (or singular value decomposition, SVD) on the covariance matrix to obtain the eigenvalues and eigenvectors.\n",
    "     - **Eigenvalues**: These represent the amount of variance explained by each principal component.\n",
    "     - **Eigenvectors**: These define the directions (principal components) along which the original features vary the most.\n",
    "\n",
    "3. **Select Principal Components**:\n",
    "   - **Explained Variance**: Determine how many principal components to retain based on the cumulative explained variance. Typically, you aim to retain enough principal components that capture a high percentage (e.g., 95% or more) of the total variance in the dataset.\n",
    "   \n",
    "   - **Dimensionality Reduction**: Project the original dataset onto the selected principal components. This reduces the dimensionality of your dataset from the original number of features to a smaller set of principal components.\n",
    "\n",
    "### Example Illustration:\n",
    "\n",
    "Suppose you have a dataset with various financial indicators (e.g., revenue, earnings per share, debt-to-equity ratio) and market trends (e.g., stock market index movements, interest rates). Here’s how you might apply PCA:\n",
    "\n",
    "1. **Dataset**:\n",
    "   - Contains financial data (e.g., revenue, earnings) and market trends (e.g., stock market indices).\n",
    "\n",
    "2. **Normalization**:\n",
    "   - Normalize each feature to have zero mean and unit variance. This step ensures that all features contribute equally to the PCA process.\n",
    "\n",
    "3. **PCA Application**:\n",
    "   - Compute the covariance matrix of the normalized dataset.\n",
    "   - Perform eigen decomposition to obtain eigenvalues and eigenvectors.\n",
    "   - Determine the number of principal components to retain based on the explained variance (e.g., retain components that explain 95% of variance).\n",
    "\n",
    "4. **Projection**:\n",
    "   - Project the original dataset onto the selected principal components to obtain a reduced-dimensional dataset.\n",
    "\n",
    "### Benefits of PCA in Stock Price Prediction:\n",
    "\n",
    "- **Dimensionality Reduction**: Reduces the number of features, which can mitigate overfitting and improve computational efficiency.\n",
    "- **Feature Interpretation**: The principal components can sometimes offer insights into the underlying structure of the data, potentially highlighting key factors influencing stock prices.\n",
    "- **Simplification**: PCA simplifies the dataset while retaining most of its variability, making it easier to work with for subsequent modeling techniques.\n",
    "\n",
    "### Implementation in Python:\n",
    "\n",
    "Here’s a basic example using Python and scikit-learn to apply PCA:\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming 'X' is your dataset containing features\n",
    "# Step 1: Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 2: Apply PCA\n",
    "pca = PCA(n_components=0.95)  # Retain components that explain 95% of variance\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "```\n",
    "\n",
    "In this example, `X_pca` will contain your dataset transformed by PCA, retaining principal components that collectively explain at least 95% of the variance in the original dataset. This reduced dataset can then be used for training predictive models for stock price prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3e69a6-f482-4261-988a-723638995b56",
   "metadata": {},
   "source": [
    "**Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773a5d80-16e6-435f-b841-adff26644130",
   "metadata": {},
   "source": [
    "To perform Min-Max scaling on the dataset [1, 5, 10, 15, 20] and transform the values to a range of -1 to 1, follow these steps:\n",
    "\n",
    "1. **Identify Min and Max Values**:\n",
    "   - Minimum value $( X_{\\text{min}} )$: 1\n",
    "   - Maximum value $( X_{\\text{max}} )$: 20\n",
    "\n",
    "2. **Apply Min-Max Scaling Formula**:\n",
    "   Min-Max scaling transforms each value $ X $ in the dataset to $ X_{\\text{scaled}} $ using the formula:\n",
    "   \n",
    "   $X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\cdot ( \\text{new_max} - \\ -1. scaled$\n",
    "   <br>\n",
    "To perform Min-Max scaling on the dataset $[1, 5, 10, 15, 20]$ and transform the values to a range of -1 to 1, let's go through the steps:\n",
    "\n",
    "1. **Identify Min and Max Values**:\n",
    "   - Minimum value $( X_{\\text{min}} )$: 1\n",
    "   - Maximum value $( X_{\\text{max}} )$: 20\n",
    "\n",
    "2. **Apply Min-Max Scaling Formula**:\n",
    "   Min-Max scaling transforms each value $ X $ in the dataset to $ X_{\\text{scaled}} $ using the formula:\n",
    "   \n",
    "   $X_{\\text{scaled}}$ = $\\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\cdot (\\text{new_max} - \\text{new_min}) + \\text{new_min}$\n",
    "   \n",
    "   where $ \\text{new_min} $ and $ \\text{new_max} $ are the desired minimum and maximum values after scaling.\n",
    "\n",
    "3. **Scale to Range [-1, 1]**:\n",
    "   - $ \\text{new_min} = -1 $\n",
    "   - $ \\text{new_max} = 1 $\n",
    "\n",
    "4. **Calculate Min-Max Scaled Values**:\n",
    "   Let's apply the formula for each value in the dataset:\n",
    "\n",
    "   - For $ X = 1 $:\n",
    "     $\n",
    "     X_{\\text{scaled}} = \\frac{1 - 1}{20 - 1} \\cdot (1 - (-1)) + (-1) = \\frac{0}{19} \\cdot 2 - 1 = -1\n",
    "     $\n",
    "\n",
    "   - For $ X = 5 $:\n",
    "     $\n",
    "     X_{\\text{scaled}} = \\frac{5 - 1}{20 - 1} \\cdot (1 - (-1)) + (-1) = \\frac{4}{19} \\cdot 2 - 1 = -\\frac{11}{19}\n",
    "     $\n",
    "\n",
    "   - For $ X = 10 $:\n",
    "     $\n",
    "     X_{\\text{scaled}} = \\frac{10 - 1}{20 - 1} \\cdot (1 - (-1)) + (-1) = \\frac{9}{19} \\cdot 2 - 1 = -\\frac{1}{19}\n",
    "     $\n",
    "\n",
    "   - For $ X = 15 $:\n",
    "     $\n",
    "     X_{\\text{scaled}} = \\frac{15 - 1}{20 - 1} \\cdot (1 - (-1)) + (-1) = \\frac{14}{19} \\cdot 2 - 1 = \\frac{7}{19}\n",
    "     $\n",
    "\n",
    "   - For $ X = 20 $:\n",
    "     $\n",
    "     X_{\\text{scaled}} = \\frac{20 - 1}{20 - 1} \\cdot (1 - (-1)) + (-1) = \\frac{19}{19} \\cdot 2 - 1 = 1\n",
    "     $\n",
    "\n",
    "5. **Resulting Min-Max Scaled Dataset**:\n",
    "   The Min-Max scaled values for the dataset $([1, 5, 10, 15, 20]$ scaled to the range [-1, 1] are:\n",
    "   $\n",
    "   [-1, -\\frac{11}{19}, -\\frac{1}{19}, \\frac{7}{19}, 1]\n",
    "   $\n",
    "\n",
    "These scaled values now lie within the desired range of -1 to 1, preserving the relative relationships and distributions of the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c233ce59-d9e5-4e21-8812-b6dc6e78e20c",
   "metadata": {},
   "source": [
    "**Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2680a9-cf88-4383-ac9e-5cf5f4df782f",
   "metadata": {},
   "source": [
    "To perform feature extraction using PCA (Principal Component Analysis) on a dataset containing features like height, weight, age, gender, and blood pressure, the goal is to reduce the dimensionality while retaining as much variance as possible. Here’s how you would approach this:\n",
    "\n",
    "### Steps to Perform PCA for Feature Extraction:\n",
    "\n",
    "1. **Data Preprocessing**:\n",
    "   - **Normalization**: Standardize or normalize the features to ensure they are on the same scale. This step is crucial as PCA is sensitive to the scale of the features.\n",
    "\n",
    "2. **Apply PCA**:\n",
    "   - **Compute Covariance Matrix**: Calculate the covariance matrix of the normalized dataset.\n",
    "\n",
    "   - **Eigen Decomposition**: Perform eigen decomposition or singular value decomposition (SVD) on the covariance matrix to obtain the eigenvalues and eigenvectors.\n",
    "\n",
    "   - **Select Principal Components**: Decide how many principal components to retain based on the explained variance. This decision is often guided by the cumulative explained variance ratio.\n",
    "\n",
    "   - **Projection**: Project the original dataset onto the selected principal components to obtain the reduced-dimensional dataset.\n",
    "\n",
    "### How Many Principal Components to Retain?\n",
    "\n",
    "The number of principal components to retain is typically determined by the cumulative explained variance ratio. This ratio tells us the proportion of the total variance in the dataset that is explained by each component. A common approach is to retain enough principal components to capture a significant percentage of the total variance, such as 95% or more.\n",
    "\n",
    "Here’s a step-by-step outline of how you might decide on the number of principal components:\n",
    "\n",
    "- **Calculate Explained Variance**: After performing PCA, you get a list of eigenvalues. The explained variance ratio for each principal component $ k $ is given by $ \\frac{\\lambda_k}{\\sum_{i=1}^{D} \\lambda_i} $, where $ \\lambda_k $ is the eigenvalue corresponding to the $ k $-th principal component and  D  is the total number of components.\n",
    "\n",
    "- **Cumulative Explained Variance**: Compute the cumulative sum of explained variances and determine how many principal components are needed to reach a certain threshold of explained variance (e.g., 95%).\n",
    "\n",
    "- **Visualize Cumulative Variance**: Plotting the cumulative explained variance can help in making an informed decision about the number of principal components to retain.\n",
    "\n",
    "### Example Decision:\n",
    "\n",
    "Suppose after applying PCA to your dataset, you find that:\n",
    "- The first principal component explains 60% of the variance.\n",
    "- The second principal component explains 25% of the variance.\n",
    "- The third principal component explains 10% of the variance.\n",
    "- The fourth and fifth principal components explain the remaining 5% of the variance combined.\n",
    "\n",
    "In this case, retaining the first three principal components would capture 95% of the variance $( 60\\% + 25\\% + 10\\% = 95\\% )$. Therefore, you might choose to retain three principal components.\n",
    "\n",
    "### Why Retain Three Principal Components?\n",
    "\n",
    "- **Dimensionality Reduction**: Retaining three principal components reduces the dataset from five dimensions (original features) to three dimensions.\n",
    "- **Variance Explanation**: By retaining three principal components, you retain most of the variance in the original dataset, which is crucial for capturing the essential information for subsequent modeling tasks.\n",
    "- **Interpretability and Efficiency**: Three principal components are easier to interpret and visualize compared to the original five features, and they also improve computational efficiency in modeling.\n",
    "\n",
    "### Summary:\n",
    "\n",
    "The choice of how many principal components to retain depends on the trade-off between dimensionality reduction and the amount of variance retained. In the context of PCA for feature extraction from a dataset with features like height, weight, age, gender, and blood pressure, retaining enough principal components to explain a high percentage of the variance (e.g., 95% or more) is typically advantageous for maintaining the integrity of the data while reducing complexity and computational overhead."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
