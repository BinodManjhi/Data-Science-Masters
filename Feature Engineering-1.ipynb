{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d831069c-abef-4eea-a10e-89d21e471fc0",
   "metadata": {},
   "source": [
    "**Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc61933f-2ae2-4aea-ad12-c693dc700d89",
   "metadata": {},
   "source": [
    "#### What are Missing Values?\n",
    "\n",
    "Missing values in a dataset are data points that are not recorded or are absent. These can occur for various reasons such as errors in data collection, data entry mistakes, or intentional omission. Missing values are typically represented by placeholders such as `NaN` (Not a Number), `None`, or empty strings in a dataset.\n",
    "\n",
    "#### Why is it Essential to Handle Missing Values?\n",
    "\n",
    "Handling missing values is crucial for several reasons:\n",
    "\n",
    "1. **Impact on Data Quality**:\n",
    "   - Missing values can lead to incorrect or biased conclusions if not handled properly.\n",
    "   - They can affect the overall integrity and reliability of the dataset.\n",
    "\n",
    "2. **Statistical Analysis**:\n",
    "   - Many statistical methods and machine learning algorithms require complete data for accurate analysis.\n",
    "   - Missing values can distort statistical measures such as mean, median, and standard deviation.\n",
    "\n",
    "3. **Model Performance**:\n",
    "   - Missing data can degrade the performance of machine learning models.\n",
    "   - Models might fail to train properly or produce inaccurate predictions if missing values are not addressed.\n",
    "\n",
    "4. **Algorithm Requirements**:\n",
    "   - Some algorithms cannot handle missing values and will raise errors if they encounter any.\n",
    "   - Proper handling ensures compatibility with a broader range of algorithms.\n",
    "\n",
    "#### Algorithms Not Affected by Missing Values\n",
    "\n",
    "Certain machine learning algorithms are inherently robust to missing values or have mechanisms to handle them. Some examples include:\n",
    "\n",
    "1. **Tree-Based Methods**:\n",
    "   - **Decision Trees**: Algorithms like CART (Classification and Regression Trees) can handle missing values by splitting the data based on the presence or absence of values.\n",
    "   - **Random Forests**: An ensemble of decision trees where each tree can handle missing values independently.\n",
    "   - **Gradient Boosting Machines (GBMs)**: Implementations like XGBoost, LightGBM, and CatBoost have built-in mechanisms to handle missing values.\n",
    "\n",
    "2. **K-Nearest Neighbors (KNN)**:\n",
    "   - In some implementations, KNN can handle missing values by using a distance metric that accommodates them or by imputing missing values with the mean or median of the nearest neighbors.\n",
    "\n",
    "3. **Naive Bayes**:\n",
    "   - The Naive Bayes algorithm can handle missing values by ignoring the missing attribute and using the available attributes for making predictions.\n",
    "\n",
    "### Example of Handling Missing Values\n",
    "\n",
    "Here's an example of handling missing values in a dataset using the pandas library in Python:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'Name': ['John', 'Anna', 'Peter', 'Linda'],\n",
    "    'Age': [28, None, 35, 32],\n",
    "    'City': ['New York', 'Paris', 'Berlin', None]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the original data\n",
    "print(\"Original Data:\")\n",
    "print(df)\n",
    "\n",
    "# Fill missing values with specific values\n",
    "df['Age'].fillna(df['Age'].median(), inplace=True)  # Fill missing ages with median age\n",
    "df['City'].fillna('Unknown', inplace=True)          # Fill missing cities with 'Unknown'\n",
    "\n",
    "# Print the cleaned data\n",
    "print(\"\\nCleaned Data:\")\n",
    "print(df)\n",
    "```\n",
    "\n",
    "### Output:\n",
    "\n",
    "```\n",
    "Original Data:\n",
    "    Name   Age      City\n",
    "0   John  28.0  New York\n",
    "1   Anna   NaN     Paris\n",
    "2  Peter  35.0    Berlin\n",
    "3  Linda  32.0      None\n",
    "\n",
    "Cleaned Data:\n",
    "    Name   Age      City\n",
    "0   John  28.0  New York\n",
    "1   Anna  31.5     Paris\n",
    "2  Peter  35.0    Berlin\n",
    "3  Linda  32.0   Unknown\n",
    "```\n",
    "\n",
    "In this example, missing values in the `Age` column are filled with the median age, and missing values in the `City` column are filled with 'Unknown'.\n",
    "\n",
    "### Summary\n",
    "\n",
    "Handling missing values is essential to maintain the integrity, reliability, and performance of data analyses and machine learning models. While some algorithms can handle missing values inherently, it's often necessary to preprocess the data to fill or impute these missing values to ensure accurate and effective analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a0bf66-158b-48f4-9013-c8cd5a865eee",
   "metadata": {},
   "source": [
    "**Q2: List down techniques used to handle missing data. Give an example of each with python code.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df359c8-b30d-4ab9-9d0c-a4f1e189b8b1",
   "metadata": {},
   "source": [
    "There are several techniques to handle missing data in datasets. The choice of technique depends on the nature of the data and the extent of missing values. Below are some common techniques along with Python code examples.\n",
    "\n",
    "#### 1. **Removing Missing Data**\n",
    "   - **Description**: Remove rows or columns with missing values. This is useful when the amount of missing data is small.\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     import pandas as pd\n",
    "\n",
    "     # Sample data\n",
    "     data = {'Name': ['John', 'Anna', 'Peter', 'Linda'],\n",
    "             'Age': [28, None, 35, 32],\n",
    "             'City': ['New York', 'Paris', 'Berlin', None]}\n",
    "     df = pd.DataFrame(data)\n",
    "\n",
    "     # Remove rows with any missing values\n",
    "     df_dropna = df.dropna()\n",
    "     print(df_dropna)\n",
    "\n",
    "     # Remove columns with any missing values\n",
    "     df_dropna_cols = df.dropna(axis=1)\n",
    "     print(df_dropna_cols)\n",
    "     ```\n",
    "\n",
    "#### 2. **Imputation with a Constant Value**\n",
    "   - **Description**: Fill missing values with a specific constant value, such as 0 or 'Unknown'.\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     # Fill missing values with a constant value\n",
    "     df_fillna = df.fillna({'Age': 0, 'City': 'Unknown'})\n",
    "     print(df_fillna)\n",
    "     ```\n",
    "\n",
    "#### 3. **Imputation with Mean/Median/Mode**\n",
    "   - **Description**: Replace missing values with the mean, median, or mode of the respective column.\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     # Fill missing values with the mean of the column\n",
    "     df['Age'].fillna(df['Age'].mean(), inplace=True)\n",
    "     print(df)\n",
    "\n",
    "     # Fill missing values with the mode of the column\n",
    "     df['City'].fillna(df['City'].mode()[0], inplace=True)\n",
    "     print(df)\n",
    "     ```\n",
    "\n",
    "#### 4. **Forward Fill and Backward Fill**\n",
    "   - **Description**: Propagate the next/previous values forward/backward to fill missing values.\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     # Forward fill\n",
    "     df_ffill = df.fillna(method='ffill')\n",
    "     print(df_ffill)\n",
    "\n",
    "     # Backward fill\n",
    "     df_bfill = df.fillna(method='bfill')\n",
    "     print(df_bfill)\n",
    "     ```\n",
    "\n",
    "#### 5. **Interpolation**\n",
    "   - **Description**: Use interpolation methods to estimate missing values.\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     # Linear interpolation\n",
    "     df['Age'] = df['Age'].interpolate()\n",
    "     print(df)\n",
    "     ```\n",
    "\n",
    "#### 6. **Using Algorithms that Handle Missing Values**\n",
    "   - **Description**: Some machine learning algorithms can handle missing values inherently, such as Decision Trees or Random Forests.\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     from sklearn.ensemble import RandomForestClassifier\n",
    "     from sklearn.model_selection import train_test_split\n",
    "\n",
    "     # Sample data\n",
    "     data = {'Feature1': [1, 2, None, 4, 5],\n",
    "             'Feature2': [5, None, 1, 2, 3],\n",
    "             'Target': [1, 0, 1, 0, 1]}\n",
    "     df = pd.DataFrame(data)\n",
    "\n",
    "     # Split data into features and target\n",
    "     X = df[['Feature1', 'Feature2']]\n",
    "     y = df['Target']\n",
    "\n",
    "     # Split into training and test sets\n",
    "     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "     # Initialize and train the model\n",
    "     model = RandomForestClassifier()\n",
    "     model.fit(X_train, y_train)\n",
    "\n",
    "     # Make predictions\n",
    "     predictions = model.predict(X_test)\n",
    "     print(predictions)\n",
    "     ```\n",
    "\n",
    "### Summary\n",
    "\n",
    "Handling missing data is a crucial step in data preprocessing. Different techniques can be applied depending on the nature of the data and the analysis requirements. Proper handling ensures that the quality of the data is maintained, which is essential for building reliable and accurate machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa2bbf9-bf18-452d-b2ff-ab08518aa0f7",
   "metadata": {},
   "source": [
    "**Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f70c246-865f-4e55-aa96-9dd82b0d24ba",
   "metadata": {},
   "source": [
    "#### What is Imbalanced Data?\n",
    "\n",
    "Imbalanced data refers to a situation where the classes in a dataset are not represented equally. This means that one class (or multiple classes) has significantly more samples than the other class(es). This is a common issue in classification problems, especially in cases where the event of interest is rare.\n",
    "\n",
    "For example, in a fraud detection dataset, the number of fraudulent transactions (positive class) is much smaller compared to the number of non-fraudulent transactions (negative class).\n",
    "\n",
    "#### Consequences of Not Handling Imbalanced Data\n",
    "\n",
    "If imbalanced data is not properly handled, it can lead to several problems:\n",
    "\n",
    "1. **Biased Model Performance**:\n",
    "   - The model may become biased towards the majority class, resulting in high accuracy but poor performance on the minority class.\n",
    "   - This can cause the model to ignore the minority class, leading to poor recall and precision for that class.\n",
    "\n",
    "2. **Misleading Accuracy**:\n",
    "   - Accuracy is not a reliable metric for imbalanced datasets. A model that predicts the majority class for all instances can still achieve high accuracy without actually learning anything useful about the minority class.\n",
    "   - For example, in a dataset with 95% of instances belonging to the majority class, a model that always predicts the majority class will have 95% accuracy but zero recall for the minority class.\n",
    "\n",
    "3. **Poor Generalization**:\n",
    "   - The model might fail to generalize well to new data, especially if the minority class is underrepresented in the training data.\n",
    "   - This leads to poor performance when the model encounters minority class samples in real-world scenarios.\n",
    "\n",
    "4. **Ineffective Predictions**:\n",
    "   - In critical applications like medical diagnosis, fraud detection, or spam detection, missing the minority class can have serious consequences, such as failing to detect a disease, missing fraudulent activities, or allowing spam emails.\n",
    "\n",
    "#### Techniques to Handle Imbalanced Data\n",
    "\n",
    "Several techniques can be employed to handle imbalanced data effectively:\n",
    "\n",
    "1. **Resampling Techniques**:\n",
    "   - **Oversampling the Minority Class**: Increase the number of samples in the minority class by duplicating them or generating new samples using techniques like SMOTE (Synthetic Minority Over-sampling Technique).\n",
    "     ```python\n",
    "     from imblearn.over_sampling import SMOTE\n",
    "     smote = SMOTE()\n",
    "     X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "     ```\n",
    "   - **Undersampling the Majority Class**: Decrease the number of samples in the majority class by randomly removing some of them.\n",
    "     ```python\n",
    "     from imblearn.under_sampling import RandomUnderSampler\n",
    "     rus = RandomUnderSampler()\n",
    "     X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "     ```\n",
    "\n",
    "2. **Using Appropriate Evaluation Metrics**:\n",
    "   - Use metrics like precision, recall, F1-score, and the area under the ROC curve (AUC-ROC) to evaluate model performance instead of accuracy.\n",
    "     ```python\n",
    "     from sklearn.metrics import classification_report, roc_auc_score\n",
    "     print(classification_report(y_test, y_pred))\n",
    "     print(\"AUC-ROC:\", roc_auc_score(y_test, y_pred_proba))\n",
    "     ```\n",
    "\n",
    "3. **Algorithmic Approaches**:\n",
    "   - Use algorithms that are inherently robust to imbalanced data or can be adjusted to account for class imbalance, such as decision trees, random forests, and gradient boosting machines.\n",
    "   - Implement cost-sensitive learning where misclassification costs are higher for the minority class.\n",
    "     ```python\n",
    "     from sklearn.ensemble import RandomForestClassifier\n",
    "     model = RandomForestClassifier(class_weight='balanced')\n",
    "     model.fit(X_train, y_train)\n",
    "     ```\n",
    "\n",
    "4. **Creating Synthetic Data**:\n",
    "   - Generate synthetic samples for the minority class using techniques like SMOTE, ADASYN (Adaptive Synthetic Sampling), or GANs (Generative Adversarial Networks).\n",
    "\n",
    "### Summary\n",
    "\n",
    "Imbalanced data is a common challenge in classification problems, where one class is underrepresented compared to others. If not handled properly, it can lead to biased models, misleading accuracy, poor generalization, and ineffective predictions. Techniques like resampling, using appropriate evaluation metrics, algorithmic adjustments, and creating synthetic data can help address the imbalance and improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736a1346-34c9-4ee1-b265-ad978b07a53b",
   "metadata": {},
   "source": [
    "**Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be64df3b-58ce-4bff-a1cd-59e6c1433f1f",
   "metadata": {},
   "source": [
    "#### Up-sampling\n",
    "\n",
    "**Up-sampling** (or oversampling) involves increasing the number of samples in the minority class to match the number of samples in the majority class. This is done to balance the class distribution in the dataset.\n",
    "\n",
    "**When Up-sampling is Required**:\n",
    "- When the minority class is significantly underrepresented.\n",
    "- To improve model performance on the minority class by providing the model with more examples from that class.\n",
    "- When you want to avoid the loss of information that comes with down-sampling the majority class.\n",
    "\n",
    "**Example of Up-sampling**:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = {\n",
    "    'feature_1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'feature_2': [10, 9, 8, 7, 6, 5, 4, 3, 2, 1],\n",
    "    'target': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = df[df.target == 0]\n",
    "df_minority = df[df.target == 1]\n",
    "\n",
    "# Up-sample minority class\n",
    "df_minority_upsampled = resample(df_minority,\n",
    "                                 replace=True,  # sample with replacement\n",
    "                                 n_samples=len(df_majority),  # to match majority class\n",
    "                                 random_state=42)  # reproducible results\n",
    "\n",
    "# Combine majority class with up-sampled minority class\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "print(df_upsampled['target'].value_counts())\n",
    "```\n",
    "\n",
    "#### Down-sampling\n",
    "\n",
    "**Down-sampling** (or undersampling) involves reducing the number of samples in the majority class to match the number of samples in the minority class. This is done to balance the class distribution in the dataset.\n",
    "\n",
    "**When Down-sampling is Required**:\n",
    "- When the majority class is significantly overrepresented.\n",
    "- To balance the dataset without increasing the size of the minority class.\n",
    "- When there is sufficient data in the majority class to allow for random sampling without losing critical information.\n",
    "\n",
    "**Example of Down-sampling**:\n",
    "\n",
    "```python\n",
    "# Down-sample majority class\n",
    "df_majority_downsampled = resample(df_majority,\n",
    "                                   replace=False,  # sample without replacement\n",
    "                                   n_samples=len(df_minority),  # to match minority class\n",
    "                                   random_state=42)  # reproducible results\n",
    "\n",
    "# Combine minority class with down-sampled majority class\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "print(df_downsampled['target'].value_counts())\n",
    "```\n",
    "\n",
    "### When Up-sampling and Down-sampling are Required\n",
    "\n",
    "**Up-sampling is typically required when**:\n",
    "- The minority class is critically underrepresented and needs more samples to be effectively learned by the model.\n",
    "- You want to avoid losing valuable majority class data which might happen during down-sampling.\n",
    "\n",
    "**Down-sampling is typically required when**:\n",
    "- The dataset is very large, and down-sampling the majority class will make training faster and less resource-intensive.\n",
    "- The majority class has redundant or non-informative samples that can be safely removed without impacting the overall model performance.\n",
    "\n",
    "### Summary\n",
    "\n",
    "Both up-sampling and down-sampling are techniques used to handle class imbalance in datasets. Up-sampling increases the number of minority class samples, while down-sampling decreases the number of majority class samples. Choosing between these techniques depends on the specific characteristics and requirements of your dataset and problem domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda53c89-5a57-4d7a-80d5-bcdf834be866",
   "metadata": {},
   "source": [
    "**Q5: What is data Augmentation? Explain SMOTE.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc410844-7949-415a-8b3b-5fdbef1c8f9d",
   "metadata": {},
   "source": [
    "#### What is Data Augmentation?\n",
    "\n",
    "Data augmentation refers to techniques used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data. Data augmentation is commonly used in various fields, including computer vision, natural language processing, and time-series analysis, to improve the performance and robustness of machine learning models.\n",
    "\n",
    "**Benefits of Data Augmentation**:\n",
    "1. **Improves Model Generalization**: Helps models generalize better to new, unseen data.\n",
    "2. **Reduces Overfitting**: By providing more training examples, it reduces the risk of overfitting.\n",
    "3. **Balances Class Distribution**: Can be used to balance class distribution in imbalanced datasets.\n",
    "\n",
    "**Examples of Data Augmentation Techniques**:\n",
    "- **Computer Vision**: Flipping, rotating, cropping, scaling, adding noise, and changing brightness or contrast of images.\n",
    "- **NLP**: Synonym replacement, random insertion, random swap, and random deletion in text data.\n",
    "- **Time-Series**: Jittering, scaling, permutation, and time warping.\n",
    "\n",
    "#### What is SMOTE?\n",
    "\n",
    "**SMOTE (Synthetic Minority Over-sampling Technique)** is a popular data augmentation technique used to address class imbalance in datasets. SMOTE works by generating synthetic samples for the minority class. Instead of simply duplicating existing minority class samples, SMOTE creates new samples by interpolating between existing minority class samples.\n",
    "\n",
    "**How SMOTE Works**:\n",
    "1. **Select a Minority Class Sample**: Randomly select a minority class sample.\n",
    "2. **Find Nearest Neighbors**: Identify the k-nearest neighbors of the selected sample.\n",
    "3. **Generate Synthetic Samples**: Randomly choose one of the k-nearest neighbors and generate a new synthetic sample by interpolating between the selected sample and its neighbor.\n",
    "\n",
    "**Benefits of SMOTE**:\n",
    "- Creates more realistic synthetic samples compared to simple duplication.\n",
    "- Improves the performance of classifiers on imbalanced datasets by providing more training examples for the minority class.\n",
    "\n",
    "**Example of Using SMOTE**:\n",
    "\n",
    "Here's how you can use SMOTE with Python:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Create an imbalanced dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.9], random_state=12)\n",
    "\n",
    "# Step 2: Convert to DataFrame for visualization\n",
    "df = pd.DataFrame(X, columns=['feature_1', 'feature_2'])\n",
    "df['target'] = y\n",
    "\n",
    "# Step 3: Visualize the original dataset\n",
    "plt.scatter(df[df['target'] == 0]['feature_1'], df[df['target'] == 0]['feature_2'], label='Majority class')\n",
    "plt.scatter(df[df['target'] == 1]['feature_1'], df[df['target'] == 1]['feature_2'], label='Minority class')\n",
    "plt.legend()\n",
    "plt.title('Original Dataset')\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Apply SMOTE to balance the dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Step 5: Convert resampled data to DataFrame for visualization\n",
    "df_resampled = pd.DataFrame(X_resampled, columns=['feature_1', 'feature_2'])\n",
    "df_resampled['target'] = y_resampled\n",
    "\n",
    "# Step 6: Visualize the resampled dataset\n",
    "plt.scatter(df_resampled[df_resampled['target'] == 0]['feature_1'], df_resampled[df_resampled['target'] == 0]['feature_2'], label='Majority class')\n",
    "plt.scatter(df_resampled[df_resampled['target'] == 1]['feature_1'], df_resampled[df_resampled['target'] == 1]['feature_2'], label='Minority class')\n",
    "plt.legend()\n",
    "plt.title('Dataset after SMOTE')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Explanation of the Code:\n",
    "\n",
    "1. **Create an Imbalanced Dataset**:\n",
    "   - We use the `make_classification` function from sklearn to create a synthetic imbalanced dataset with a 90:10 class ratio.\n",
    "\n",
    "2. **Visualize the Original Dataset**:\n",
    "   - Plot the original dataset to see the imbalance between the majority and minority classes.\n",
    "\n",
    "3. **Apply SMOTE**:\n",
    "   - Use the `SMOTE` function from the imbalanced-learn library to generate synthetic samples for the minority class and balance the dataset.\n",
    "\n",
    "4. **Visualize the Resampled Dataset**:\n",
    "   - Plot the resampled dataset to observe the balanced class distribution after applying SMOTE.\n",
    "\n",
    "### Summary\n",
    "\n",
    "Data augmentation techniques, such as SMOTE, are essential for improving the performance and robustness of machine learning models, especially when dealing with imbalanced datasets. SMOTE generates synthetic samples for the minority class by interpolating between existing samples, thereby providing a more balanced training dataset for the classifier. This helps in achieving better model performance and reducing bias towards the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af596fea-7183-4cad-a4a4-d052cead49e5",
   "metadata": {},
   "source": [
    "**Q6: What are outliers in a dataset? Why is it essential to handle outliers?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1e4865-ba6f-4ece-9ed4-ae1d26507c01",
   "metadata": {},
   "source": [
    "Outliers are data points that differ significantly from the majority of the data. These extreme values can arise from measurement errors, data entry errors, or natural variability in the data.\n",
    "\n",
    "Handling outliers is crucial for several reasons:\n",
    "\n",
    "1. **Impact on Statistical Measures**:\n",
    "   - Outliers can distort statistical measures like mean and standard deviation, leading to a misleading representation of the dataset.\n",
    "\n",
    "2. **Model Performance**:\n",
    "   - Machine learning models, especially those based on distance metrics (e.g., k-nearest neighbors, clustering algorithms), can be adversely affected by outliers.\n",
    "   - Models may overfit to the outliers, resulting in poor generalization to new data.\n",
    "\n",
    "3. **Data Quality**:\n",
    "   - Outliers might indicate errors in the data collection process that need to be addressed to ensure the dataset's integrity.\n",
    "   - Removing or correcting outliers can lead to a cleaner and more reliable dataset.\n",
    "\n",
    "### Identifying Outliers\n",
    "\n",
    "Outliers can be identified using various methods:\n",
    "\n",
    "- **Visual Inspection**: Box plots, scatter plots, and histograms can visually reveal outliers.\n",
    "- **Statistical Methods**: Methods such as Z-scores or the Interquartile Range (IQR) can be used to identify outliers quantitatively.\n",
    "\n",
    "### Handling Outliers\n",
    "\n",
    "There are several strategies to handle outliers:\n",
    "\n",
    "1. **Removing Outliers**: Simply exclude the outlier data points from the dataset if they are errors or if the dataset is large enough to handle the loss of some data.\n",
    "2. **Transforming Data**: Apply transformations like logarithmic or square root transformations to reduce the impact of outliers.\n",
    "3. **Capping/Flooring**: Set a threshold and cap values beyond a certain point to the threshold value.\n",
    "4. **Imputation**: Replace outliers with a value based on other observations, such as the mean or median.\n",
    "\n",
    "### Example of Handling Outliers Using Python\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data with outliers\n",
    "data = {'feature': [1, 2, 2, 3, 2, 1, 3, 100, 2, 3, 2, 1, 2]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Visualize the data using a box plot\n",
    "plt.boxplot(df['feature'])\n",
    "plt.title('Box Plot of Feature with Outliers')\n",
    "plt.show()\n",
    "\n",
    "# Detecting outliers using IQR method\n",
    "Q1 = df['feature'].quantile(0.25)\n",
    "Q3 = df['feature'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the lower and upper bound\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identify outliers\n",
    "outliers = df[(df['feature'] < lower_bound) | (df['feature'] > upper_bound)]\n",
    "print(\"Outliers detected:\\n\", outliers)\n",
    "\n",
    "# Handling outliers by removing them\n",
    "df_no_outliers = df[(df['feature'] >= lower_bound) & (df['feature'] <= upper_bound)]\n",
    "print(\"Data after removing outliers:\\n\", df_no_outliers)\n",
    "\n",
    "# Visualize the data without outliers using a box plot\n",
    "plt.boxplot(df_no_outliers['feature'])\n",
    "plt.title('Box Plot of Feature without Outliers')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "This code demonstrates how to identify and handle outliers in a dataset using the IQR method. It visualizes the data before and after removing outliers, providing a clear illustration of the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be25533-d2c1-4b65-b5da-b9c84ec1b9de",
   "metadata": {},
   "source": [
    "**Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1380ec4d-ded3-4e95-a403-94449bd6bb00",
   "metadata": {},
   "source": [
    "When dealing with missing data in a dataset, it's crucial to handle it appropriately to ensure the accuracy and reliability of your analysis. Here are several techniques to handle missing data:\n",
    "\n",
    "### 1. **Removing Missing Data**\n",
    "\n",
    "- **Complete Case Analysis**: Remove rows with any missing values.\n",
    "  ```python\n",
    "  df_cleaned = df.dropna()\n",
    "  ```\n",
    "- **Listwise Deletion**: Remove rows where specific columns have missing values.\n",
    "  ```python\n",
    "  df_cleaned = df.dropna(subset=['column1', 'column2'])\n",
    "  ```\n",
    "\n",
    "### 2. **Imputing Missing Data**\n",
    "\n",
    "- **Mean/Median/Mode Imputation**:\n",
    "  - Replace missing values with the mean (for numerical data), median (for skewed numerical data), or mode (for categorical data).\n",
    "  ```python\n",
    "  df['column1'].fillna(df['column1'].mean(), inplace=True)\n",
    "  df['column2'].fillna(df['column2'].median(), inplace=True)\n",
    "  df['column3'].fillna(df['column3'].mode()[0], inplace=True)\n",
    "  ```\n",
    "\n",
    "- **Forward Fill**:\n",
    "  - Fill missing values using the previous value in the column.\n",
    "  ```python\n",
    "  df_ffill = df.fillna(method='ffill')\n",
    "  ```\n",
    "\n",
    "- **Backward Fill**:\n",
    "  - Fill missing values using the next value in the column.\n",
    "  ```python\n",
    "  df_bfill = df.fillna(method='bfill')\n",
    "  ```\n",
    "\n",
    "- **Interpolation**:\n",
    "  - Use interpolation methods to estimate and fill missing values.\n",
    "  ```python\n",
    "  df['column1'] = df['column1'].interpolate(method='linear')\n",
    "  ```\n",
    "\n",
    "### 3. **Advanced Imputation Techniques**\n",
    "\n",
    "- **K-Nearest Neighbors (KNN) Imputation**:\n",
    "  - Use the KNN algorithm to impute missing values based on the values of the nearest neighbors.\n",
    "  ```python\n",
    "  from sklearn.impute import KNNImputer\n",
    "  imputer = KNNImputer(n_neighbors=5)\n",
    "  df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "  ```\n",
    "\n",
    "- **Multiple Imputation**:\n",
    "  - Use multiple imputation techniques, such as the MICE (Multiple Imputation by Chained Equations) method.\n",
    "  ```python\n",
    "  from sklearn.experimental import enable_iterative_imputer\n",
    "  from sklearn.impute import IterativeImputer\n",
    "  imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "  df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "  ```\n",
    "\n",
    "### 4. **Using Algorithms that Handle Missing Values Natively**\n",
    "\n",
    "Some algorithms can handle missing values without any preprocessing. These include:\n",
    "- Decision Trees and ensemble methods like Random Forest and Gradient Boosting.\n",
    "- Algorithms like XGBoost and LightGBM have built-in mechanisms to handle missing data.\n",
    "\n",
    "### Example Code for Handling Missing Data\n",
    "\n",
    "Here's an example demonstrating some of these techniques:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Sample data with missing values\n",
    "data = {\n",
    "    'age': [25, np.nan, 35, 45, np.nan, 50],\n",
    "    'salary': [50000, 60000, np.nan, 80000, 75000, np.nan],\n",
    "    'gender': ['Male', 'Female', np.nan, 'Male', 'Female', 'Male']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 1. Mean Imputation\n",
    "df['age'].fillna(df['age'].mean(), inplace=True)\n",
    "print(\"After Mean Imputation:\\n\", df)\n",
    "\n",
    "# 2. Forward Fill\n",
    "df_ffill = df.fillna(method='ffill')\n",
    "print(\"After Forward Fill:\\n\", df_ffill)\n",
    "\n",
    "# 3. KNN Imputation\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "df[['age', 'salary']] = imputer.fit_transform(df[['age', 'salary']])\n",
    "print(\"After KNN Imputation:\\n\", df)\n",
    "\n",
    "# 4. Multiple Imputation using Iterative Imputer\n",
    "imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "df[['age', 'salary']] = imputer.fit_transform(df[['age', 'salary']])\n",
    "print(\"After Multiple Imputation:\\n\", df)\n",
    "```\n",
    "\n",
    "By employing these techniques, you can handle missing data effectively, ensuring that your analysis remains robust and reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb307ad1-b083-43c4-abe7-c420bbda3d14",
   "metadata": {},
   "source": [
    "**Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b81b292-c72c-4c8c-9eef-85aecc2f61fc",
   "metadata": {},
   "source": [
    "To determine if the missing data in your dataset is missing at random or if there is a pattern to the missing data, you can use the following strategies:\n",
    "\n",
    "### 1. **Visual Inspection**\n",
    "\n",
    "- **Heatmap of Missing Values**:\n",
    "  - Create a heatmap to visualize the distribution of missing values.\n",
    "  ```python\n",
    "  import seaborn as sns\n",
    "  import matplotlib.pyplot as plt\n",
    "  sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
    "  plt.title('Heatmap of Missing Values')\n",
    "  plt.show()\n",
    "  ```\n",
    "\n",
    "- **Bar Plot of Missing Values**:\n",
    "  - Use bar plots to show the percentage of missing values in each column.\n",
    "  ```python\n",
    "  missing_values = df.isnull().sum() / len(df) * 100\n",
    "  missing_values.plot(kind='bar')\n",
    "  plt.title('Percentage of Missing Values by Column')\n",
    "  plt.ylabel('Percentage')\n",
    "  plt.show()\n",
    "  ```\n",
    "\n",
    "### 2. **Statistical Tests**\n",
    "\n",
    "- **Little's MCAR Test**:\n",
    "  - Little's Missing Completely at Random (MCAR) test can be used to statistically test if the data is missing completely at random.\n",
    "  - Unfortunately, this test is not directly available in standard Python libraries but can be performed using specialized statistical software like SPSS.\n",
    "\n",
    "### 3. **Correlation Analysis**\n",
    "\n",
    "- **Correlation with Other Variables**:\n",
    "  - Calculate the correlation between the missingness of one variable and the presence of missing values in other variables or the actual values of other variables.\n",
    "  ```python\n",
    "  missing_correlation = df.isnull().corr()\n",
    "  sns.heatmap(missing_correlation, annot=True, cmap='coolwarm')\n",
    "  plt.title('Correlation of Missing Values')\n",
    "  plt.show()\n",
    "  ```\n",
    "\n",
    "- **Pairwise Comparison**:\n",
    "  - Compare distributions of variables with and without missing values in another variable.\n",
    "  ```python\n",
    "  sns.boxplot(x=df['missing_column'].isnull(), y=df['other_column'])\n",
    "  plt.title('Comparison of Other Column with Missing and Non-Missing Values')\n",
    "  plt.show()\n",
    "  ```\n",
    "\n",
    "### 4. **Pattern Analysis**\n",
    "\n",
    "- **Missingness Patterns**:\n",
    "  - Use libraries like `missingno` to visualize patterns of missingness.\n",
    "  ```python\n",
    "  import missingno as msno\n",
    "  msno.matrix(df)\n",
    "  plt.title('Missing Data Matrix')\n",
    "  plt.show()\n",
    "\n",
    "  msno.heatmap(df)\n",
    "  plt.title('Missing Data Heatmap')\n",
    "  plt.show()\n",
    "  ```\n",
    "\n",
    "### 5. **Subset Analysis**\n",
    "\n",
    "- **Create Subsets**:\n",
    "  - Create subsets of the data based on missing and non-missing values and compare them.\n",
    "  ```python\n",
    "  df_missing = df[df['column_with_missing'].isnull()]\n",
    "  df_non_missing = df[df['column_with_missing'].notnull()]\n",
    "  ```\n",
    "\n",
    "- **Compare Statistics**:\n",
    "  - Compare summary statistics (mean, median, variance) of the subsets.\n",
    "  ```python\n",
    "  print(\"Summary statistics of missing subset:\\n\", df_missing.describe())\n",
    "  print(\"Summary statistics of non-missing subset:\\n\", df_non_missing.describe())\n",
    "  ```\n",
    "\n",
    "### 6. **Machine Learning Approaches**\n",
    "\n",
    "- **Predict Missingness**:\n",
    "  - Use machine learning models to predict missing values. If the model performs well, it suggests that there is a pattern to the missingness.\n",
    "  ```python\n",
    "  from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "  df['missing_indicator'] = df['target_column'].isnull().astype(int)\n",
    "  df_features = df.drop(columns=['target_column', 'missing_indicator'])\n",
    "  df_target = df['missing_indicator']\n",
    "\n",
    "  model = RandomForestClassifier()\n",
    "  model.fit(df_features, df_target)\n",
    "  feature_importances = pd.Series(model.feature_importances_, index=df_features.columns)\n",
    "  feature_importances.plot(kind='bar')\n",
    "  plt.title('Feature Importances for Predicting Missingness')\n",
    "  plt.show()\n",
    "  ```\n",
    "\n",
    "By using these strategies, you can gain insights into the nature of the missing data and determine whether it is missing at random (MAR), missing completely at random (MCAR), or missing not at random (MNAR). This understanding will guide you in selecting the appropriate method to handle the missing data in your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5fc492-dcfa-4cb2-8188-cabc58443ca3",
   "metadata": {},
   "source": [
    "**Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bca8f91-6128-4134-8fb8-9f03ca357519",
   "metadata": {},
   "source": [
    "When dealing with an imbalanced dataset, especially in a critical domain like medical diagnosis, it's important to use appropriate strategies to evaluate the performance of your machine learning model. Here are some strategies you can use:\n",
    "\n",
    "### 1. **Evaluation Metrics**\n",
    "Using standard accuracy can be misleading in imbalanced datasets. Instead, consider the following metrics:\n",
    "\n",
    "- **Precision**: The ratio of true positives to the sum of true and false positives. It indicates the accuracy of the positive predictions.\n",
    "  \\[\n",
    "  \\text{Precision} = \\frac{TP}{TP + FP}\n",
    "  \\]\n",
    "\n",
    "- **Recall (Sensitivity or True Positive Rate)**: The ratio of true positives to the sum of true positives and false negatives. It indicates how well the model can identify positive cases.\n",
    "  \\[\n",
    "  \\text{Recall} = \\frac{TP}{TP + FN}\n",
    "  \\]\n",
    "\n",
    "- **F1 Score**: The harmonic mean of precision and recall, providing a single metric that balances both.\n",
    "  \\[\n",
    "  \\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "  \\]\n",
    "\n",
    "- **Specificity (True Negative Rate)**: The ratio of true negatives to the sum of true negatives and false positives. It indicates how well the model can identify negative cases.\n",
    "  \\[\n",
    "  \\text{Specificity} = \\frac{TN}{TN + FP}\n",
    "  \\]\n",
    "\n",
    "- **ROC Curve and AUC (Area Under the Curve)**: The ROC curve plots the true positive rate against the false positive rate at various threshold settings. The AUC provides an aggregate measure of performance across all classification thresholds.\n",
    "  ```python\n",
    "  from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "  fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)\n",
    "  roc_auc = auc(fpr, tpr)\n",
    "\n",
    "  plt.figure()\n",
    "  plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "  plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "  plt.xlim([0.0, 1.0])\n",
    "  plt.ylim([0.0, 1.05])\n",
    "  plt.xlabel('False Positive Rate')\n",
    "  plt.ylabel('True Positive Rate')\n",
    "  plt.title('Receiver Operating Characteristic')\n",
    "  plt.legend(loc=\"lower right\")\n",
    "  plt.show()\n",
    "  ```\n",
    "\n",
    "- **Precision-Recall Curve**: Especially useful for imbalanced datasets, this curve plots precision against recall at various threshold settings.\n",
    "  ```python\n",
    "  from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "  precision, recall, thresholds = precision_recall_curve(y_true, y_pred_proba)\n",
    "\n",
    "  plt.figure()\n",
    "  plt.plot(recall, precision, color='blue', lw=2)\n",
    "  plt.xlabel('Recall')\n",
    "  plt.ylabel('Precision')\n",
    "  plt.title('Precision-Recall Curve')\n",
    "  plt.show()\n",
    "  ```\n",
    "\n",
    "### 2. **Resampling Techniques**\n",
    "\n",
    "- **Over-sampling the Minority Class**: Increase the number of instances in the minority class by duplicating examples or generating synthetic examples using techniques like SMOTE (Synthetic Minority Over-sampling Technique).\n",
    "  ```python\n",
    "  from imblearn.over_sampling import SMOTE\n",
    "\n",
    "  smote = SMOTE()\n",
    "  X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "  ```\n",
    "\n",
    "- **Under-sampling the Majority Class**: Reduce the number of instances in the majority class to balance the dataset.\n",
    "  ```python\n",
    "  from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "  undersampler = RandomUnderSampler()\n",
    "  X_resampled, y_resampled = undersampler.fit_resample(X, y)\n",
    "  ```\n",
    "\n",
    "- **Combination of Over-sampling and Under-sampling**: Use a balanced approach where you both under-sample the majority class and over-sample the minority class to achieve a balanced dataset.\n",
    "\n",
    "### 3. **Algorithmic Approaches**\n",
    "\n",
    "- **Use Models that Handle Imbalance Well**: Some algorithms are more robust to class imbalance, such as decision trees, random forests, and gradient boosting machines.\n",
    "\n",
    "- **Adjust Class Weights**: Modify the algorithm to give more importance to the minority class by adjusting class weights.\n",
    "  ```python\n",
    "  from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "  model = RandomForestClassifier(class_weight='balanced')\n",
    "  model.fit(X_train, y_train)\n",
    "  ```\n",
    "\n",
    "### 4. **Cross-Validation**\n",
    "\n",
    "- **Stratified K-Fold Cross-Validation**: Ensure that each fold of the cross-validation process has the same proportion of classes as the original dataset.\n",
    "  ```python\n",
    "  from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "  skf = StratifiedKFold(n_splits=5)\n",
    "  for train_index, test_index in skf.split(X, y):\n",
    "      X_train, X_test = X[train_index], X[test_index]\n",
    "      y_train, y_test = y[train_index], y[test_index]\n",
    "  ```\n",
    "\n",
    "### 5. **Threshold Tuning**\n",
    "\n",
    "- **Adjust Decision Threshold**: Fine-tune the threshold for classification to achieve a balance between precision and recall.\n",
    "  ```python\n",
    "  from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "  precision, recall, thresholds = precision_recall_curve(y_true, y_pred_proba)\n",
    "  optimal_idx = np.argmax(2 * precision * recall / (precision + recall))\n",
    "  optimal_threshold = thresholds[optimal_idx]\n",
    "  ```\n",
    "\n",
    "### Example Code for Evaluating Performance on Imbalanced Data\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming y_true are the true labels and y_pred_proba are the predicted probabilities\n",
    "y_true = np.array([...])\n",
    "y_pred_proba = np.array([...])\n",
    "\n",
    "# Calculate precision, recall, F1-score\n",
    "print(classification_report(y_true, y_pred_proba > 0.5))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_proba > 0.5)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, color='blue', lw=2)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Using these strategies, you can better evaluate and improve the performance of your machine learning model on an imbalanced dataset, ensuring it performs well even on the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa37504-330e-47da-aaf8-7f59434e4062",
   "metadata": {},
   "source": [
    "**Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3f99aa-ddb5-43b4-8cd2-c11cc258ab5b",
   "metadata": {},
   "source": [
    "When dealing with an unbalanced dataset where the bulk of customers report being satisfied, it is crucial to balance the dataset to ensure that your machine learning model performs well on both the majority and minority classes. Here are some methods to balance the dataset and down-sample the majority class:\n",
    "\n",
    "### 1. Down-sampling the Majority Class\n",
    "\n",
    "Down-sampling involves reducing the number of instances in the majority class to balance the dataset. This can be done randomly or strategically.\n",
    "\n",
    "#### Random Under-Sampling\n",
    "Randomly select a subset of the majority class to match the size of the minority class.\n",
    "\n",
    "```python\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate the majority and minority classes\n",
    "majority_class = df[df['satisfaction'] == 'satisfied']\n",
    "minority_class = df[df['satisfaction'] == 'not_satisfied']\n",
    "\n",
    "# Down-sample the majority class\n",
    "majority_downsampled = resample(majority_class, \n",
    "                                replace=False,    # sample without replacement\n",
    "                                n_samples=len(minority_class),  # match minority class size\n",
    "                                random_state=42)  # reproducible results\n",
    "\n",
    "# Combine the minority class with the downsampled majority class\n",
    "balanced_df = pd.concat([minority_class, majority_downsampled])\n",
    "```\n",
    "\n",
    "#### Cluster-Based Under-Sampling\n",
    "Use clustering algorithms to identify representative samples of the majority class.\n",
    "\n",
    "```python\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "# Assume X and y are your features and target\n",
    "cc = ClusterCentroids(random_state=42)\n",
    "X_resampled, y_resampled = cc.fit_resample(X, y)\n",
    "```\n",
    "\n",
    "### 2. Over-sampling the Minority Class\n",
    "\n",
    "Over-sampling involves increasing the number of instances in the minority class to balance the dataset.\n",
    "\n",
    "#### Random Over-Sampling\n",
    "Randomly duplicate examples from the minority class.\n",
    "\n",
    "```python\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Over-sample the minority class\n",
    "minority_oversampled = resample(minority_class, \n",
    "                                replace=True,     # sample with replacement\n",
    "                                n_samples=len(majority_class),  # match majority class size\n",
    "                                random_state=42)  # reproducible results\n",
    "\n",
    "# Combine the oversampled minority class with the majority class\n",
    "balanced_df = pd.concat([majority_class, minority_oversampled])\n",
    "```\n",
    "\n",
    "#### SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "Generate synthetic samples for the minority class.\n",
    "\n",
    "```python\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "```\n",
    "\n",
    "### 3. Combination of Over-sampling and Under-sampling\n",
    "\n",
    "Combine both techniques to balance the dataset.\n",
    "\n",
    "```python\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "smote_enn = SMOTEENN(random_state=42)\n",
    "X_resampled, y_resampled = smote_enn.fit_resample(X, y)\n",
    "```\n",
    "\n",
    "### 4. Using Advanced Algorithms\n",
    "\n",
    "Some algorithms are inherently better at handling imbalanced datasets. Examples include decision trees, random forests, and gradient boosting algorithms. \n",
    "\n",
    "### 5. Adjusting Class Weights\n",
    "\n",
    "Modify the algorithm to give more importance to the minority class by adjusting class weights.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "### Example Code for Down-Sampling and Over-Sampling\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Assume df is your DataFrame and 'satisfaction' is your target column\n",
    "\n",
    "# Separate the majority and minority classes\n",
    "majority_class = df[df['satisfaction'] == 'satisfied']\n",
    "minority_class = df[df['satisfaction'] == 'not_satisfied']\n",
    "\n",
    "# Down-sample the majority class\n",
    "majority_downsampled = resample(majority_class, \n",
    "                                replace=False,    # sample without replacement\n",
    "                                n_samples=len(minority_class),  # match minority class size\n",
    "                                random_state=42)  # reproducible results\n",
    "\n",
    "# Combine the minority class with the downsampled majority class\n",
    "downsampled_df = pd.concat([minority_class, majority_downsampled])\n",
    "\n",
    "# Over-sample the minority class using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X = df.drop(columns='satisfaction')\n",
    "y = df['satisfaction']\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Create a DataFrame from the resampled data\n",
    "oversampled_df = pd.concat([pd.DataFrame(X_resampled), pd.DataFrame(y_resampled, columns=['satisfaction'])], axis=1)\n",
    "```\n",
    "\n",
    "By using these methods, you can balance the dataset and ensure that your machine learning model performs well on both the majority and minority classes. This helps in providing a more accurate and fair analysis of customer satisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587c5fca-a02b-440c-a78d-f218bf04cb59",
   "metadata": {},
   "source": [
    "**Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e340e80-00f2-472e-82ad-0fdf11f2b34b",
   "metadata": {},
   "source": [
    "When dealing with an unbalanced dataset where the bulk of customers report being satisfied, it is crucial to balance the dataset to ensure that your machine learning model performs well on both the majority and minority classes. Here are some methods to balance the dataset and down-sample the majority class:\n",
    "\n",
    "### 1. Down-sampling the Majority Class\n",
    "\n",
    "Down-sampling involves reducing the number of instances in the majority class to balance the dataset. This can be done randomly or strategically.\n",
    "\n",
    "#### Random Under-Sampling\n",
    "Randomly select a subset of the majority class to match the size of the minority class.\n",
    "\n",
    "```python\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate the majority and minority classes\n",
    "majority_class = df[df['satisfaction'] == 'satisfied']\n",
    "minority_class = df[df['satisfaction'] == 'not_satisfied']\n",
    "\n",
    "# Down-sample the majority class\n",
    "majority_downsampled = resample(majority_class, \n",
    "                                replace=False,    # sample without replacement\n",
    "                                n_samples=len(minority_class),  # match minority class size\n",
    "                                random_state=42)  # reproducible results\n",
    "\n",
    "# Combine the minority class with the downsampled majority class\n",
    "balanced_df = pd.concat([minority_class, majority_downsampled])\n",
    "```\n",
    "\n",
    "#### Cluster-Based Under-Sampling\n",
    "Use clustering algorithms to identify representative samples of the majority class.\n",
    "\n",
    "```python\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "# Assume X and y are your features and target\n",
    "cc = ClusterCentroids(random_state=42)\n",
    "X_resampled, y_resampled = cc.fit_resample(X, y)\n",
    "```\n",
    "\n",
    "### 2. Over-sampling the Minority Class\n",
    "\n",
    "Over-sampling involves increasing the number of instances in the minority class to balance the dataset.\n",
    "\n",
    "#### Random Over-Sampling\n",
    "Randomly duplicate examples from the minority class.\n",
    "\n",
    "```python\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Over-sample the minority class\n",
    "minority_oversampled = resample(minority_class, \n",
    "                                replace=True,     # sample with replacement\n",
    "                                n_samples=len(majority_class),  # match majority class size\n",
    "                                random_state=42)  # reproducible results\n",
    "\n",
    "# Combine the oversampled minority class with the majority class\n",
    "balanced_df = pd.concat([majority_class, minority_oversampled])\n",
    "```\n",
    "\n",
    "#### SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "Generate synthetic samples for the minority class.\n",
    "\n",
    "```python\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "```\n",
    "\n",
    "### 3. Combination of Over-sampling and Under-sampling\n",
    "\n",
    "Combine both techniques to balance the dataset.\n",
    "\n",
    "```python\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "smote_enn = SMOTEENN(random_state=42)\n",
    "X_resampled, y_resampled = smote_enn.fit_resample(X, y)\n",
    "```\n",
    "\n",
    "### 4. Using Advanced Algorithms\n",
    "\n",
    "Some algorithms are inherently better at handling imbalanced datasets. Examples include decision trees, random forests, and gradient boosting algorithms. \n",
    "\n",
    "### 5. Adjusting Class Weights\n",
    "\n",
    "Modify the algorithm to give more importance to the minority class by adjusting class weights.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "### Example Code for Down-Sampling and Over-Sampling\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Assume df is your DataFrame and 'satisfaction' is your target column\n",
    "\n",
    "# Separate the majority and minority classes\n",
    "majority_class = df[df['satisfaction'] == 'satisfied']\n",
    "minority_class = df[df['satisfaction'] == 'not_satisfied']\n",
    "\n",
    "# Down-sample the majority class\n",
    "majority_downsampled = resample(majority_class, \n",
    "                                replace=False,    # sample without replacement\n",
    "                                n_samples=len(minority_class),  # match minority class size\n",
    "                                random_state=42)  # reproducible results\n",
    "\n",
    "# Combine the minority class with the downsampled majority class\n",
    "downsampled_df = pd.concat([minority_class, majority_downsampled])\n",
    "\n",
    "# Over-sample the minority class using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X = df.drop(columns='satisfaction')\n",
    "y = df['satisfaction']\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Create a DataFrame from the resampled data\n",
    "oversampled_df = pd.concat([pd.DataFrame(X_resampled), pd.DataFrame(y_resampled, columns=['satisfaction'])], axis=1)\n",
    "```\n",
    "\n",
    "By using these methods, you can balance the dataset and ensure that your machine learning model performs well on both the majority and minority classes. This helps in providing a more accurate and fair analysis of customer satisfaction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
